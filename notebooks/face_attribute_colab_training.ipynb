{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "A100",
   "mount_file_id": "11B52lrzNa7f3sOkD-t3wqV1SydG-eAgO",
   "authorship_tag": "ABX9TyOQvVrkpgVT7UEgbxasDA5y"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "#Read large dataset"
   ],
   "metadata": {
    "id": "7zCNYwV90VbU"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Upload dataset into gg drive prior to executing the following command\n",
    "!unzip /content/drive/MyDrive/celeb_A.zip > /dev/null\n",
    "# !unzip /content/drive/MyDrive/small_celeb_A.zip > /dev/null"
   ],
   "metadata": {
    "id": "d4C3Su3YhjLm",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1708410785796,
     "user_tz": -420,
     "elapsed": 33048,
     "user": {
      "displayName": "Trong Nguyen Duc",
      "userId": "13500150630151633478"
     }
    }
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Install packages"
   ],
   "metadata": {
    "id": "gkK5Wkqj8Imz"
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install torchsummary torchmetrics torcheval python-box[all]~=7.0"
   ],
   "metadata": {
    "id": "zfA7Nehc8Ity",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1708410792048,
     "user_tz": -420,
     "elapsed": 6283,
     "user": {
      "displayName": "Trong Nguyen Duc",
      "userId": "13500150630151633478"
     }
    },
    "outputId": "3f8c27ba-a823-4e13-f100-3966d2acb32f"
   },
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: torchsummary in /usr/local/lib/python3.10/dist-packages (1.5.1)\n",
      "Collecting torchmetrics\n",
      "  Downloading torchmetrics-1.3.1-py3-none-any.whl (840 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m840.4/840.4 kB\u001B[0m \u001B[31m13.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting torcheval\n",
      "  Downloading torcheval-0.0.7-py3-none-any.whl (179 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m179.2/179.2 kB\u001B[0m \u001B[31m24.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: python-box[all]~=7.0 in /usr/local/lib/python3.10/dist-packages (7.1.1)\n",
      "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.25.2)\n",
      "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (23.2)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.1.0+cu121)\n",
      "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
      "  Downloading lightning_utilities-0.10.1-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torcheval) (4.9.0)\n",
      "Collecting ruamel.yaml>=0.17 (from python-box[all]~=7.0)\n",
      "  Downloading ruamel.yaml-0.18.6-py3-none-any.whl (117 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m117.8/117.8 kB\u001B[0m \u001B[31m16.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from python-box[all]~=7.0) (0.10.2)\n",
      "Requirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from python-box[all]~=7.0) (1.0.7)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (67.7.2)\n",
      "Collecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml>=0.17->python-box[all]~=7.0)\n",
      "  Downloading ruamel.yaml.clib-0.2.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (526 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m526.7/526.7 kB\u001B[0m \u001B[31m50.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.13.1)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2023.6.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\n",
      "Installing collected packages: torcheval, ruamel.yaml.clib, lightning-utilities, ruamel.yaml, torchmetrics\n",
      "Successfully installed lightning-utilities-0.10.1 ruamel.yaml-0.18.6 ruamel.yaml.clib-0.2.8 torcheval-0.0.7 torchmetrics-1.3.1\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training configs"
   ],
   "metadata": {
    "id": "4uXBffot6mPh"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from box import Box\n",
    "options = {\n",
    "    \"EXPERIMENTS\":\n",
    "    {\n",
    "        \"PROJECT_NAME\": \"face_attribute\"\n",
    "    },\n",
    "\n",
    "    \"DATA\":\n",
    "    {\n",
    "        \"DATASET_NAME\": \"celeb_A\",\n",
    "        \"INPUT_SHAPE\": [224, 224, 3],\n",
    "        \"TRAIN_SIZE\": 0.9,\n",
    "        \"BATCH_SIZE\": 500,\n",
    "        \"NUM_WORKERS\": 4\n",
    "    },\n",
    "\n",
    "    \"CHECKPOINT\":\n",
    "    {\n",
    "        \"SAVE\": True,\n",
    "        \"LOAD\": False,\n",
    "        \"SAVE_ALL\": False,\n",
    "        \"RESUME_NAME\": \"epoch_1.pt\"\n",
    "    },\n",
    "\n",
    "    \"EPOCH\":\n",
    "    {\n",
    "        \"START\": 1,\n",
    "        \"EPOCHS\": 50\n",
    "    },\n",
    "\n",
    "    \"METRICS\":\n",
    "    {\n",
    "        \"NAME_LIST\": [\"BinaryAccuracy\", \"BinaryF1Score\"],\n",
    "        \"ARGS\":\n",
    "        {\n",
    "            \"0\":\n",
    "            {\n",
    "               \"threshold\": 0.5\n",
    "            },\n",
    "\n",
    "            \"1\":\n",
    "            {\n",
    "               \"threshold\": 0.5\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "\n",
    "    \"SOLVER\":\n",
    "    {\n",
    "        \"MODEL\":\n",
    "        {\n",
    "            \"BASE\": \"vgg\",\n",
    "            \"NAME\": \"vgg13\",\n",
    "            \"PRETRAINED\": False,\n",
    "            \"ARGS\":\n",
    "            {\n",
    "                \"dropout\": 0.5,\n",
    "                \"num_classes\": 1\n",
    "            }\n",
    "        },\n",
    "\n",
    "        \"OPTIMIZER\":\n",
    "        {\n",
    "            \"NAME\": \"Adam\",\n",
    "            \"ARGS\":\n",
    "            {\n",
    "                \"lr\": 1e-7,\n",
    "                \"amsgrad\": True\n",
    "            }\n",
    "        },\n",
    "\n",
    "        \"LOSS\":\n",
    "        {\n",
    "            \"NAME\": \"BCELoss\",\n",
    "            \"ARGS\":\n",
    "            {\n",
    "                \"reduction\": \"mean\"\n",
    "            }\n",
    "        },\n",
    "\n",
    "        \"EARLY_STOPPING\":\n",
    "        {\n",
    "            \"PATIENCE\": 5,\n",
    "            \"MIN_DELTA\": 0\n",
    "        }\n",
    "    },\n",
    "\n",
    "    \"MISC\":\n",
    "    {\n",
    "        \"SEED\": 12345,\n",
    "        \"APPLY_EARLY_STOPPING\": True,\n",
    "        \"CUDA\": True\n",
    "    }\n",
    "}\n",
    "options = Box(options)"
   ],
   "metadata": {
    "id": "8mLbUsW07bGM",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1708410792049,
     "user_tz": -420,
     "elapsed": 25,
     "user": {
      "displayName": "Trong Nguyen Duc",
      "userId": "13500150630151633478"
     }
    }
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "VGG base"
   ],
   "metadata": {
    "id": "lHNtLDLw7eao"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "\n",
    "__all__ = [\n",
    "    'VGG', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn',\n",
    "    'vgg19_bn', 'vgg19', \"get_vgg_model\"\n",
    "]\n",
    "\n",
    "\n",
    "model_urls = {\n",
    "    'vgg11': 'https://download.pytorch.org/models/vgg11-bbd30ac9.pth',\n",
    "    'vgg13': 'https://download.pytorch.org/models/vgg13-c768596a.pth',\n",
    "    'vgg16': 'https://download.pytorch.org/models/vgg16-397923af.pth',\n",
    "    'vgg19': 'https://download.pytorch.org/models/vgg19-dcbb9e9d.pth',\n",
    "    'vgg11_bn': 'https://download.pytorch.org/models/vgg11_bn-6002323d.pth',\n",
    "    'vgg13_bn': 'https://download.pytorch.org/models/vgg13_bn-abd245e5.pth',\n",
    "    'vgg16_bn': 'https://download.pytorch.org/models/vgg16_bn-6c64b313.pth',\n",
    "    'vgg19_bn': 'https://download.pytorch.org/models/vgg19_bn-c79401a0.pth',\n",
    "}\n",
    "\n",
    "cfg = {\n",
    "    'A': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'B': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'D': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    'E': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
    "}\n",
    "\n",
    "\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self, features, dropout=.5, num_classes=1000, init_weights=True):\n",
    "        super(VGG, self).__init__()\n",
    "        self.features = features\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 7 * 7, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(dropout),\n",
    "\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(dropout),\n",
    "\n",
    "            nn.Linear(4096, 2048),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(dropout),\n",
    "\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(dropout),\n",
    "\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(dropout),\n",
    "\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "        if init_weights:\n",
    "            self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(\n",
    "                    m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "def make_layers(cfg, batch_norm=False):\n",
    "    layers = []\n",
    "    in_channels = 3\n",
    "    for v in cfg:\n",
    "        if v == 'M':\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "        else:\n",
    "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
    "            if batch_norm:\n",
    "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "            else:\n",
    "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "            in_channels = v\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def vgg11(pretrained=False, **kwargs):\n",
    "    \"\"\"VGG 11-layer model (configuration \"A\")\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    if pretrained:\n",
    "        kwargs['init_weights'] = False\n",
    "    model = VGG(make_layers(cfg['A']), **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['vgg11']))\n",
    "    return model\n",
    "\n",
    "\n",
    "def vgg11_bn(pretrained=False, **kwargs):\n",
    "    \"\"\"VGG 11-layer model (configuration \"A\") with batch normalization\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    if pretrained:\n",
    "        kwargs['init_weights'] = False\n",
    "    model = VGG(make_layers(cfg['A'], batch_norm=True), **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['vgg11_bn']))\n",
    "    return model\n",
    "\n",
    "\n",
    "def vgg13(pretrained=False, **kwargs):\n",
    "    \"\"\"VGG 13-layer model (configuration \"B\")\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    if pretrained:\n",
    "        kwargs['init_weights'] = False\n",
    "    model = VGG(make_layers(cfg['B']), **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['vgg13']))\n",
    "    return model\n",
    "\n",
    "\n",
    "def vgg13_bn(pretrained=False, **kwargs):\n",
    "    \"\"\"VGG 13-layer model (configuration \"B\") with batch normalization\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    if pretrained:\n",
    "        kwargs['init_weights'] = False\n",
    "    model = VGG(make_layers(cfg['B'], batch_norm=True), **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['vgg13_bn']))\n",
    "    return model\n",
    "\n",
    "\n",
    "def vgg16(pretrained=False, **kwargs):\n",
    "    \"\"\"VGG 16-layer model (configuration \"D\")\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    if pretrained:\n",
    "        kwargs['init_weights'] = False\n",
    "    model = VGG(make_layers(cfg['D']), **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['vgg16']))\n",
    "    return model\n",
    "\n",
    "\n",
    "def vgg16_bn(pretrained=False, **kwargs):\n",
    "    \"\"\"VGG 16-layer model (configuration \"D\") with batch normalization\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    if pretrained:\n",
    "        kwargs['init_weights'] = False\n",
    "    model = VGG(make_layers(cfg['D'], batch_norm=True), **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['vgg16_bn']))\n",
    "    return model\n",
    "\n",
    "\n",
    "def vgg19(pretrained=False, **kwargs):\n",
    "    \"\"\"VGG 19-layer model (configuration \"E\")\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    if pretrained:\n",
    "        kwargs['init_weights'] = False\n",
    "    model = VGG(make_layers(cfg['E']), **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['vgg19']))\n",
    "    return model\n",
    "\n",
    "\n",
    "def vgg19_bn(pretrained=False, **kwargs):\n",
    "    \"\"\"VGG 19-layer model (configuration 'E') with batch normalization\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    if pretrained:\n",
    "        kwargs['init_weights'] = False\n",
    "    model = VGG(make_layers(cfg['E'], batch_norm=True), **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['vgg19_bn']))\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_vgg_model(cuda: bool, name: str, pretrained: bool = True, model_state_dict: dict = None, **kwargs):\n",
    "    models = {\n",
    "        \"vgg13\": vgg13,\n",
    "        \"vgg16\": vgg16,\n",
    "        \"vgg19\": vgg19,\n",
    "        \"vgg13_bn\": vgg13_bn,\n",
    "        \"vgg16_bn\": vgg16_bn,\n",
    "        \"vgg19_bn\": vgg19_bn,\n",
    "    }\n",
    "    assert name in models.keys(), \"Your selected vgg model derivative is unavailable\"\n",
    "\n",
    "    model = models[name](pretrained=pretrained, **kwargs)\n",
    "\n",
    "    if model_state_dict:\n",
    "        print(\"Loading pretrained model...\")\n",
    "        model.load_state_dict(model_state_dict)\n",
    "        print(\"Finished.\")\n",
    "    else:\n",
    "        print(\"Initializing parameters...\")\n",
    "        for para in model.parameters():\n",
    "            if para.dim() > 1:\n",
    "                nn.init.xavier_uniform_(para)\n",
    "        print(\"Finished.\")\n",
    "\n",
    "    if cuda:\n",
    "        model = model.to(\"cuda\")\n",
    "    return model\n"
   ],
   "metadata": {
    "id": "JgyqDtVQ7diG",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1708410796556,
     "user_tz": -420,
     "elapsed": 4531,
     "user": {
      "displayName": "Trong Nguyen Duc",
      "userId": "13500150630151633478"
     }
    }
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "ResNet base"
   ],
   "metadata": {
    "id": "Q8J8LE-V7nCj"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from functools import partial\n",
    "from typing import Any, Callable, List, Optional, Type, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "\n",
    "from torchvision.utils import _log_api_usage_once\n",
    "from torchvision.models._meta import _IMAGENET_CATEGORIES\n",
    "from torchvision.transforms._presets import ImageClassification\n",
    "from torchvision.models._api import register_model, Weights, WeightsEnum\n",
    "from torchvision.models._utils import _ovewrite_named_param, handle_legacy_interface\n",
    "\n",
    "\n",
    "__all__ = [\n",
    "    \"ResNet\",\n",
    "    \"ResNet18_Weights\",\n",
    "    \"ResNet34_Weights\",\n",
    "    \"ResNet50_Weights\",\n",
    "    \"ResNet101_Weights\",\n",
    "    \"ResNet152_Weights\",\n",
    "    \"ResNeXt50_32X4D_Weights\",\n",
    "    \"ResNeXt101_32X8D_Weights\",\n",
    "    \"ResNeXt101_64X4D_Weights\",\n",
    "    \"Wide_ResNet50_2_Weights\",\n",
    "    \"Wide_ResNet101_2_Weights\",\n",
    "    \"resnet18\",\n",
    "    \"resnet34\",\n",
    "    \"resnet50\",\n",
    "    \"resnet101\",\n",
    "    \"resnet152\",\n",
    "    \"resnext50_32x4d\",\n",
    "    \"resnext101_32x8d\",\n",
    "    \"resnext101_64x4d\",\n",
    "    \"wide_resnet50_2\",\n",
    "    \"wide_resnet101_2\",\n",
    "    \"get_resnet_model\"\n",
    "]\n",
    "\n",
    "\n",
    "def conv3x3(in_planes: int, out_planes: int, stride: int = 1, groups: int = 1, dilation: int = 1) -> nn.Conv2d:\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(\n",
    "        in_planes,\n",
    "        out_planes,\n",
    "        kernel_size=3,\n",
    "        stride=stride,\n",
    "        padding=dilation,\n",
    "        groups=groups,\n",
    "        bias=False,\n",
    "        dilation=dilation,\n",
    "    )\n",
    "\n",
    "\n",
    "def conv1x1(in_planes: int, out_planes: int, stride: int = 1) -> nn.Conv2d:\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion: int = 1\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        inplanes: int,\n",
    "        planes: int,\n",
    "        stride: int = 1,\n",
    "        downsample: Optional[nn.Module] = None,\n",
    "        groups: int = 1,\n",
    "        base_width: int = 64,\n",
    "        dilation: int = 1,\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        if groups != 1 or base_width != 64:\n",
    "            raise ValueError(\"BasicBlock only supports groups=1 and base_width=64\")\n",
    "        if dilation > 1:\n",
    "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
    "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = norm_layer(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = norm_layer(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n",
    "    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\n",
    "    # according to \"Deep residual learning for image recognition\" https://arxiv.org/abs/1512.03385.\n",
    "    # This variant is also known as ResNet V1.5 and improves accuracy according to\n",
    "    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.\n",
    "\n",
    "    expansion: int = 4\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        inplanes: int,\n",
    "        planes: int,\n",
    "        stride: int = 1,\n",
    "        downsample: Optional[nn.Module] = None,\n",
    "        groups: int = 1,\n",
    "        base_width: int = 64,\n",
    "        dilation: int = 1,\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        width = int(planes * (base_width / 64.0)) * groups\n",
    "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv1x1(inplanes, width)\n",
    "        self.bn1 = norm_layer(width)\n",
    "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
    "        self.bn2 = norm_layer(width)\n",
    "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
    "        self.bn3 = norm_layer(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        block: Type[Union[BasicBlock, Bottleneck]],\n",
    "        layers: List[int],\n",
    "        num_classes: int = 1000,\n",
    "        zero_init_residual: bool = False,\n",
    "        groups: int = 1,\n",
    "        width_per_group: int = 64,\n",
    "        replace_stride_with_dilation: Optional[List[bool]] = None,\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
    "    ) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "        _log_api_usage_once(self)\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        self._norm_layer = norm_layer\n",
    "\n",
    "        self.inplanes = 64\n",
    "        self.dilation = 1\n",
    "        if replace_stride_with_dilation is None:\n",
    "            # each element in the tuple indicates if we should replace\n",
    "            # the 2x2 stride with a dilated convolution instead\n",
    "            replace_stride_with_dilation = [False, False, False]\n",
    "        if len(replace_stride_with_dilation) != 3:\n",
    "            raise ValueError(\n",
    "                \"replace_stride_with_dilation should be None \"\n",
    "                f\"or a 3-element tuple, got {replace_stride_with_dilation}\"\n",
    "            )\n",
    "        self.groups = groups\n",
    "        self.base_width = width_per_group\n",
    "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = norm_layer(self.inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, dilate=replace_stride_with_dilation[0])\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2, dilate=replace_stride_with_dilation[1])\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2, dilate=replace_stride_with_dilation[2])\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, Bottleneck) and m.bn3.weight is not None:\n",
    "                    nn.init.constant_(m.bn3.weight, 0)  # type: ignore[arg-type]\n",
    "                elif isinstance(m, BasicBlock) and m.bn2.weight is not None:\n",
    "                    nn.init.constant_(m.bn2.weight, 0)  # type: ignore[arg-type]\n",
    "\n",
    "    def _make_layer(\n",
    "        self,\n",
    "        block: Type[Union[BasicBlock, Bottleneck]],\n",
    "        planes: int,\n",
    "        blocks: int,\n",
    "        stride: int = 1,\n",
    "        dilate: bool = False,\n",
    "    ) -> nn.Sequential:\n",
    "        norm_layer = self._norm_layer\n",
    "        downsample = None\n",
    "        previous_dilation = self.dilation\n",
    "        if dilate:\n",
    "            self.dilation *= stride\n",
    "            stride = 1\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
    "                norm_layer(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(\n",
    "            block(\n",
    "                self.inplanes, planes, stride, downsample, self.groups, self.base_width, previous_dilation, norm_layer\n",
    "            )\n",
    "        )\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(\n",
    "                block(\n",
    "                    self.inplanes,\n",
    "                    planes,\n",
    "                    groups=self.groups,\n",
    "                    base_width=self.base_width,\n",
    "                    dilation=self.dilation,\n",
    "                    norm_layer=norm_layer,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _forward_impl(self, x: Tensor) -> Tensor:\n",
    "        # See note [TorchScript super()]\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self._forward_impl(x)\n",
    "\n",
    "\n",
    "def _resnet(\n",
    "    block: Type[Union[BasicBlock, Bottleneck]],\n",
    "    layers: List[int],\n",
    "    weights: Optional[WeightsEnum],\n",
    "    progress: bool,\n",
    "    **kwargs: Any,\n",
    ") -> ResNet:\n",
    "    if weights is not None:\n",
    "        _ovewrite_named_param(kwargs, \"num_classes\", len(weights.meta[\"categories\"]))\n",
    "\n",
    "    model = ResNet(block, layers, **kwargs)\n",
    "\n",
    "    if weights is not None:\n",
    "        model.load_state_dict(weights.get_state_dict(progress=progress, check_hash=True))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "_COMMON_META = {\n",
    "    \"min_size\": (1, 1),\n",
    "    \"categories\": _IMAGENET_CATEGORIES,\n",
    "}\n",
    "\n",
    "\n",
    "class ResNet18_Weights(WeightsEnum):\n",
    "    IMAGENET1K_V1 = Weights(\n",
    "        url=\"https://download.pytorch.org/models/resnet18-f37072fd.pth\",\n",
    "        transforms=partial(ImageClassification, crop_size=224),\n",
    "        meta={\n",
    "            **_COMMON_META,\n",
    "            \"num_params\": 11689512,\n",
    "            \"recipe\": \"https://github.com/pytorch/vision/tree/main/references/classification#resnet\",\n",
    "            \"_metrics\": {\n",
    "                \"ImageNet-1K\": {\n",
    "                    \"acc@1\": 69.758,\n",
    "                    \"acc@5\": 89.078,\n",
    "                }\n",
    "            },\n",
    "            \"_ops\": 1.814,\n",
    "            \"_file_size\": 44.661,\n",
    "            \"_docs\": \"\"\"These weights reproduce closely the results of the paper using a simple training recipe.\"\"\",\n",
    "        },\n",
    "    )\n",
    "    DEFAULT = IMAGENET1K_V1\n",
    "\n",
    "\n",
    "class ResNet34_Weights(WeightsEnum):\n",
    "    IMAGENET1K_V1 = Weights(\n",
    "        url=\"https://download.pytorch.org/models/resnet34-b627a593.pth\",\n",
    "        transforms=partial(ImageClassification, crop_size=224),\n",
    "        meta={\n",
    "            **_COMMON_META,\n",
    "            \"num_params\": 21797672,\n",
    "            \"recipe\": \"https://github.com/pytorch/vision/tree/main/references/classification#resnet\",\n",
    "            \"_metrics\": {\n",
    "                \"ImageNet-1K\": {\n",
    "                    \"acc@1\": 73.314,\n",
    "                    \"acc@5\": 91.420,\n",
    "                }\n",
    "            },\n",
    "            \"_ops\": 3.664,\n",
    "            \"_file_size\": 83.275,\n",
    "            \"_docs\": \"\"\"These weights reproduce closely the results of the paper using a simple training recipe.\"\"\",\n",
    "        },\n",
    "    )\n",
    "    DEFAULT = IMAGENET1K_V1\n",
    "\n",
    "\n",
    "class ResNet50_Weights(WeightsEnum):\n",
    "    IMAGENET1K_V1 = Weights(\n",
    "        url=\"https://download.pytorch.org/models/resnet50-0676ba61.pth\",\n",
    "        transforms=partial(ImageClassification, crop_size=224),\n",
    "        meta={\n",
    "            **_COMMON_META,\n",
    "            \"num_params\": 25557032,\n",
    "            \"recipe\": \"https://github.com/pytorch/vision/tree/main/references/classification#resnet\",\n",
    "            \"_metrics\": {\n",
    "                \"ImageNet-1K\": {\n",
    "                    \"acc@1\": 76.130,\n",
    "                    \"acc@5\": 92.862,\n",
    "                }\n",
    "            },\n",
    "            \"_ops\": 4.089,\n",
    "            \"_file_size\": 97.781,\n",
    "            \"_docs\": \"\"\"These weights reproduce closely the results of the paper using a simple training recipe.\"\"\",\n",
    "        },\n",
    "    )\n",
    "    IMAGENET1K_V2 = Weights(\n",
    "        url=\"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\",\n",
    "        transforms=partial(ImageClassification, crop_size=224, resize_size=232),\n",
    "        meta={\n",
    "            **_COMMON_META,\n",
    "            \"num_params\": 25557032,\n",
    "            \"recipe\": \"https://github.com/pytorch/vision/issues/3995#issuecomment-1013906621\",\n",
    "            \"_metrics\": {\n",
    "                \"ImageNet-1K\": {\n",
    "                    \"acc@1\": 80.858,\n",
    "                    \"acc@5\": 95.434,\n",
    "                }\n",
    "            },\n",
    "            \"_ops\": 4.089,\n",
    "            \"_file_size\": 97.79,\n",
    "            \"_docs\": \"\"\"\n",
    "                These weights improve upon the results of the original paper by using TorchVision's `new training recipe\n",
    "                <https://pytorch.org/blog/how-to-train-state-of-the-art-models-using-torchvision-latest-primitives/>`_.\n",
    "            \"\"\",\n",
    "        },\n",
    "    )\n",
    "    DEFAULT = IMAGENET1K_V2\n",
    "\n",
    "\n",
    "class ResNet101_Weights(WeightsEnum):\n",
    "    IMAGENET1K_V1 = Weights(\n",
    "        url=\"https://download.pytorch.org/models/resnet101-63fe2227.pth\",\n",
    "        transforms=partial(ImageClassification, crop_size=224),\n",
    "        meta={\n",
    "            **_COMMON_META,\n",
    "            \"num_params\": 44549160,\n",
    "            \"recipe\": \"https://github.com/pytorch/vision/tree/main/references/classification#resnet\",\n",
    "            \"_metrics\": {\n",
    "                \"ImageNet-1K\": {\n",
    "                    \"acc@1\": 77.374,\n",
    "                    \"acc@5\": 93.546,\n",
    "                }\n",
    "            },\n",
    "            \"_ops\": 7.801,\n",
    "            \"_file_size\": 170.511,\n",
    "            \"_docs\": \"\"\"These weights reproduce closely the results of the paper using a simple training recipe.\"\"\",\n",
    "        },\n",
    "    )\n",
    "    IMAGENET1K_V2 = Weights(\n",
    "        url=\"https://download.pytorch.org/models/resnet101-cd907fc2.pth\",\n",
    "        transforms=partial(ImageClassification, crop_size=224, resize_size=232),\n",
    "        meta={\n",
    "            **_COMMON_META,\n",
    "            \"num_params\": 44549160,\n",
    "            \"recipe\": \"https://github.com/pytorch/vision/issues/3995#new-recipe\",\n",
    "            \"_metrics\": {\n",
    "                \"ImageNet-1K\": {\n",
    "                    \"acc@1\": 81.886,\n",
    "                    \"acc@5\": 95.780,\n",
    "                }\n",
    "            },\n",
    "            \"_ops\": 7.801,\n",
    "            \"_file_size\": 170.53,\n",
    "            \"_docs\": \"\"\"\n",
    "                These weights improve upon the results of the original paper by using TorchVision's `new training recipe\n",
    "                <https://pytorch.org/blog/how-to-train-state-of-the-art-models-using-torchvision-latest-primitives/>`_.\n",
    "            \"\"\",\n",
    "        },\n",
    "    )\n",
    "    DEFAULT = IMAGENET1K_V2\n",
    "\n",
    "\n",
    "class ResNet152_Weights(WeightsEnum):\n",
    "    IMAGENET1K_V1 = Weights(\n",
    "        url=\"https://download.pytorch.org/models/resnet152-394f9c45.pth\",\n",
    "        transforms=partial(ImageClassification, crop_size=224),\n",
    "        meta={\n",
    "            **_COMMON_META,\n",
    "            \"num_params\": 60192808,\n",
    "            \"recipe\": \"https://github.com/pytorch/vision/tree/main/references/classification#resnet\",\n",
    "            \"_metrics\": {\n",
    "                \"ImageNet-1K\": {\n",
    "                    \"acc@1\": 78.312,\n",
    "                    \"acc@5\": 94.046,\n",
    "                }\n",
    "            },\n",
    "            \"_ops\": 11.514,\n",
    "            \"_file_size\": 230.434,\n",
    "            \"_docs\": \"\"\"These weights reproduce closely the results of the paper using a simple training recipe.\"\"\",\n",
    "        },\n",
    "    )\n",
    "    IMAGENET1K_V2 = Weights(\n",
    "        url=\"https://download.pytorch.org/models/resnet152-f82ba261.pth\",\n",
    "        transforms=partial(ImageClassification, crop_size=224, resize_size=232),\n",
    "        meta={\n",
    "            **_COMMON_META,\n",
    "            \"num_params\": 60192808,\n",
    "            \"recipe\": \"https://github.com/pytorch/vision/issues/3995#new-recipe\",\n",
    "            \"_metrics\": {\n",
    "                \"ImageNet-1K\": {\n",
    "                    \"acc@1\": 82.284,\n",
    "                    \"acc@5\": 96.002,\n",
    "                }\n",
    "            },\n",
    "            \"_ops\": 11.514,\n",
    "            \"_file_size\": 230.474,\n",
    "            \"_docs\": \"\"\"\n",
    "                These weights improve upon the results of the original paper by using TorchVision's `new training recipe\n",
    "                <https://pytorch.org/blog/how-to-train-state-of-the-art-models-using-torchvision-latest-primitives/>`_.\n",
    "            \"\"\",\n",
    "        },\n",
    "    )\n",
    "    DEFAULT = IMAGENET1K_V2\n",
    "\n",
    "\n",
    "class ResNeXt50_32X4D_Weights(WeightsEnum):\n",
    "    IMAGENET1K_V1 = Weights(\n",
    "        url=\"https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth\",\n",
    "        transforms=partial(ImageClassification, crop_size=224),\n",
    "        meta={\n",
    "            **_COMMON_META,\n",
    "            \"num_params\": 25028904,\n",
    "            \"recipe\": \"https://github.com/pytorch/vision/tree/main/references/classification#resnext\",\n",
    "            \"_metrics\": {\n",
    "                \"ImageNet-1K\": {\n",
    "                    \"acc@1\": 77.618,\n",
    "                    \"acc@5\": 93.698,\n",
    "                }\n",
    "            },\n",
    "            \"_ops\": 4.23,\n",
    "            \"_file_size\": 95.789,\n",
    "            \"_docs\": \"\"\"These weights reproduce closely the results of the paper using a simple training recipe.\"\"\",\n",
    "        },\n",
    "    )\n",
    "    IMAGENET1K_V2 = Weights(\n",
    "        url=\"https://download.pytorch.org/models/resnext50_32x4d-1a0047aa.pth\",\n",
    "        transforms=partial(ImageClassification, crop_size=224, resize_size=232),\n",
    "        meta={\n",
    "            **_COMMON_META,\n",
    "            \"num_params\": 25028904,\n",
    "            \"recipe\": \"https://github.com/pytorch/vision/issues/3995#new-recipe\",\n",
    "            \"_metrics\": {\n",
    "                \"ImageNet-1K\": {\n",
    "                    \"acc@1\": 81.198,\n",
    "                    \"acc@5\": 95.340,\n",
    "                }\n",
    "            },\n",
    "            \"_ops\": 4.23,\n",
    "            \"_file_size\": 95.833,\n",
    "            \"_docs\": \"\"\"\n",
    "                These weights improve upon the results of the original paper by using TorchVision's `new training recipe\n",
    "                <https://pytorch.org/blog/how-to-train-state-of-the-art-models-using-torchvision-latest-primitives/>`_.\n",
    "            \"\"\",\n",
    "        },\n",
    "    )\n",
    "    DEFAULT = IMAGENET1K_V2\n",
    "\n",
    "\n",
    "class ResNeXt101_32X8D_Weights(WeightsEnum):\n",
    "    IMAGENET1K_V1 = Weights(\n",
    "        url=\"https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth\",\n",
    "        transforms=partial(ImageClassification, crop_size=224),\n",
    "        meta={\n",
    "            **_COMMON_META,\n",
    "            \"num_params\": 88791336,\n",
    "            \"recipe\": \"https://github.com/pytorch/vision/tree/main/references/classification#resnext\",\n",
    "            \"_metrics\": {\n",
    "                \"ImageNet-1K\": {\n",
    "                    \"acc@1\": 79.312,\n",
    "                    \"acc@5\": 94.526,\n",
    "                }\n",
    "            },\n",
    "            \"_ops\": 16.414,\n",
    "            \"_file_size\": 339.586,\n",
    "            \"_docs\": \"\"\"These weights reproduce closely the results of the paper using a simple training recipe.\"\"\",\n",
    "        },\n",
    "    )\n",
    "    IMAGENET1K_V2 = Weights(\n",
    "        url=\"https://download.pytorch.org/models/resnext101_32x8d-110c445d.pth\",\n",
    "        transforms=partial(ImageClassification, crop_size=224, resize_size=232),\n",
    "        meta={\n",
    "            **_COMMON_META,\n",
    "            \"num_params\": 88791336,\n",
    "            \"recipe\": \"https://github.com/pytorch/vision/issues/3995#new-recipe-with-fixres\",\n",
    "            \"_metrics\": {\n",
    "                \"ImageNet-1K\": {\n",
    "                    \"acc@1\": 82.834,\n",
    "                    \"acc@5\": 96.228,\n",
    "                }\n",
    "            },\n",
    "            \"_ops\": 16.414,\n",
    "            \"_file_size\": 339.673,\n",
    "            \"_docs\": \"\"\"\n",
    "                These weights improve upon the results of the original paper by using TorchVision's `new training recipe\n",
    "                <https://pytorch.org/blog/how-to-train-state-of-the-art-models-using-torchvision-latest-primitives/>`_.\n",
    "            \"\"\",\n",
    "        },\n",
    "    )\n",
    "    DEFAULT = IMAGENET1K_V2\n",
    "\n",
    "\n",
    "class ResNeXt101_64X4D_Weights(WeightsEnum):\n",
    "    IMAGENET1K_V1 = Weights(\n",
    "        url=\"https://download.pytorch.org/models/resnext101_64x4d-173b62eb.pth\",\n",
    "        transforms=partial(ImageClassification, crop_size=224, resize_size=232),\n",
    "        meta={\n",
    "            **_COMMON_META,\n",
    "            \"num_params\": 83455272,\n",
    "            \"recipe\": \"https://github.com/pytorch/vision/pull/5935\",\n",
    "            \"_metrics\": {\n",
    "                \"ImageNet-1K\": {\n",
    "                    \"acc@1\": 83.246,\n",
    "                    \"acc@5\": 96.454,\n",
    "                }\n",
    "            },\n",
    "            \"_ops\": 15.46,\n",
    "            \"_file_size\": 319.318,\n",
    "            \"_docs\": \"\"\"\n",
    "                These weights were trained from scratch by using TorchVision's `new training recipe\n",
    "                <https://pytorch.org/blog/how-to-train-state-of-the-art-models-using-torchvision-latest-primitives/>`_.\n",
    "            \"\"\",\n",
    "        },\n",
    "    )\n",
    "    DEFAULT = IMAGENET1K_V1\n",
    "\n",
    "\n",
    "class Wide_ResNet50_2_Weights(WeightsEnum):\n",
    "    IMAGENET1K_V1 = Weights(\n",
    "        url=\"https://download.pytorch.org/models/wide_resnet50_2-95faca4d.pth\",\n",
    "        transforms=partial(ImageClassification, crop_size=224),\n",
    "        meta={\n",
    "            **_COMMON_META,\n",
    "            \"num_params\": 68883240,\n",
    "            \"recipe\": \"https://github.com/pytorch/vision/pull/912#issue-445437439\",\n",
    "            \"_metrics\": {\n",
    "                \"ImageNet-1K\": {\n",
    "                    \"acc@1\": 78.468,\n",
    "                    \"acc@5\": 94.086,\n",
    "                }\n",
    "            },\n",
    "            \"_ops\": 11.398,\n",
    "            \"_file_size\": 131.82,\n",
    "            \"_docs\": \"\"\"These weights reproduce closely the results of the paper using a simple training recipe.\"\"\",\n",
    "        },\n",
    "    )\n",
    "    IMAGENET1K_V2 = Weights(\n",
    "        url=\"https://download.pytorch.org/models/wide_resnet50_2-9ba9bcbe.pth\",\n",
    "        transforms=partial(ImageClassification, crop_size=224, resize_size=232),\n",
    "        meta={\n",
    "            **_COMMON_META,\n",
    "            \"num_params\": 68883240,\n",
    "            \"recipe\": \"https://github.com/pytorch/vision/issues/3995#new-recipe-with-fixres\",\n",
    "            \"_metrics\": {\n",
    "                \"ImageNet-1K\": {\n",
    "                    \"acc@1\": 81.602,\n",
    "                    \"acc@5\": 95.758,\n",
    "                }\n",
    "            },\n",
    "            \"_ops\": 11.398,\n",
    "            \"_file_size\": 263.124,\n",
    "            \"_docs\": \"\"\"\n",
    "                These weights improve upon the results of the original paper by using TorchVision's `new training recipe\n",
    "                <https://pytorch.org/blog/how-to-train-state-of-the-art-models-using-torchvision-latest-primitives/>`_.\n",
    "            \"\"\",\n",
    "        },\n",
    "    )\n",
    "    DEFAULT = IMAGENET1K_V2\n",
    "\n",
    "\n",
    "class Wide_ResNet101_2_Weights(WeightsEnum):\n",
    "    IMAGENET1K_V1 = Weights(\n",
    "        url=\"https://download.pytorch.org/models/wide_resnet101_2-32ee1156.pth\",\n",
    "        transforms=partial(ImageClassification, crop_size=224),\n",
    "        meta={\n",
    "            **_COMMON_META,\n",
    "            \"num_params\": 126886696,\n",
    "            \"recipe\": \"https://github.com/pytorch/vision/pull/912#issue-445437439\",\n",
    "            \"_metrics\": {\n",
    "                \"ImageNet-1K\": {\n",
    "                    \"acc@1\": 78.848,\n",
    "                    \"acc@5\": 94.284,\n",
    "                }\n",
    "            },\n",
    "            \"_ops\": 22.753,\n",
    "            \"_file_size\": 242.896,\n",
    "            \"_docs\": \"\"\"These weights reproduce closely the results of the paper using a simple training recipe.\"\"\",\n",
    "        },\n",
    "    )\n",
    "    IMAGENET1K_V2 = Weights(\n",
    "        url=\"https://download.pytorch.org/models/wide_resnet101_2-d733dc28.pth\",\n",
    "        transforms=partial(ImageClassification, crop_size=224, resize_size=232),\n",
    "        meta={\n",
    "            **_COMMON_META,\n",
    "            \"num_params\": 126886696,\n",
    "            \"recipe\": \"https://github.com/pytorch/vision/issues/3995#new-recipe\",\n",
    "            \"_metrics\": {\n",
    "                \"ImageNet-1K\": {\n",
    "                    \"acc@1\": 82.510,\n",
    "                    \"acc@5\": 96.020,\n",
    "                }\n",
    "            },\n",
    "            \"_ops\": 22.753,\n",
    "            \"_file_size\": 484.747,\n",
    "            \"_docs\": \"\"\"\n",
    "                These weights improve upon the results of the original paper by using TorchVision's `new training recipe\n",
    "                <https://pytorch.org/blog/how-to-train-state-of-the-art-models-using-torchvision-latest-primitives/>`_.\n",
    "            \"\"\",\n",
    "        },\n",
    "    )\n",
    "    DEFAULT = IMAGENET1K_V2\n",
    "\n",
    "\n",
    "\n",
    "@handle_legacy_interface(weights=(\"pretrained\", ResNet18_Weights.IMAGENET1K_V1))\n",
    "def resnet18(*, weights: Optional[ResNet18_Weights] = None, progress: bool = True, **kwargs: Any) -> ResNet:\n",
    "    \"\"\"ResNet-18 from `Deep Residual Learning for Image Recognition <https://arxiv.org/abs/1512.03385>`__.\n",
    "\n",
    "    Args:\n",
    "        weights (:class:`~torchvision.models.ResNet18_Weights`, optional): The\n",
    "            pretrained weights to use. See\n",
    "            :class:`~torchvision.models.ResNet18_Weights` below for\n",
    "            more details, and possible values. By default, no pre-trained\n",
    "            weights are used.\n",
    "        progress (bool, optional): If True, displays a progress bar of the\n",
    "            download to stderr. Default is True.\n",
    "        **kwargs: parameters passed to the ``torchvision.models.resnet.ResNet``\n",
    "            base class. Please refer to the `source code\n",
    "            <https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py>`_\n",
    "            for more details about this class.\n",
    "\n",
    "    .. autoclass:: torchvision.models.ResNet18_Weights\n",
    "        :members:\n",
    "    \"\"\"\n",
    "    weights = ResNet18_Weights.verify(weights)\n",
    "\n",
    "    return _resnet(BasicBlock, [2, 2, 2, 2], weights, progress, **kwargs)\n",
    "\n",
    "\n",
    "\n",
    "@handle_legacy_interface(weights=(\"pretrained\", ResNet34_Weights.IMAGENET1K_V1))\n",
    "def resnet34(*, weights: Optional[ResNet34_Weights] = None, progress: bool = True, **kwargs: Any) -> ResNet:\n",
    "    \"\"\"ResNet-34 from `Deep Residual Learning for Image Recognition <https://arxiv.org/abs/1512.03385>`__.\n",
    "\n",
    "    Args:\n",
    "        weights (:class:`~torchvision.models.ResNet34_Weights`, optional): The\n",
    "            pretrained weights to use. See\n",
    "            :class:`~torchvision.models.ResNet34_Weights` below for\n",
    "            more details, and possible values. By default, no pre-trained\n",
    "            weights are used.\n",
    "        progress (bool, optional): If True, displays a progress bar of the\n",
    "            download to stderr. Default is True.\n",
    "        **kwargs: parameters passed to the ``torchvision.models.resnet.ResNet``\n",
    "            base class. Please refer to the `source code\n",
    "            <https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py>`_\n",
    "            for more details about this class.\n",
    "\n",
    "    .. autoclass:: torchvision.models.ResNet34_Weights\n",
    "        :members:\n",
    "    \"\"\"\n",
    "    weights = ResNet34_Weights.verify(weights)\n",
    "\n",
    "    return _resnet(BasicBlock, [3, 4, 6, 3], weights, progress, **kwargs)\n",
    "\n",
    "\n",
    "\n",
    "@handle_legacy_interface(weights=(\"pretrained\", ResNet50_Weights.IMAGENET1K_V1))\n",
    "def resnet50(*, weights: Optional[ResNet50_Weights] = None, progress: bool = True, **kwargs: Any) -> ResNet:\n",
    "    \"\"\"ResNet-50 from `Deep Residual Learning for Image Recognition <https://arxiv.org/abs/1512.03385>`__.\n",
    "\n",
    "    .. note::\n",
    "       The bottleneck of TorchVision places the stride for downsampling to the second 3x3\n",
    "       convolution while the original paper places it to the first 1x1 convolution.\n",
    "       This variant improves the accuracy and is known as `ResNet V1.5\n",
    "       <https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch>`_.\n",
    "\n",
    "    Args:\n",
    "        weights (:class:`~torchvision.models.ResNet50_Weights`, optional): The\n",
    "            pretrained weights to use. See\n",
    "            :class:`~torchvision.models.ResNet50_Weights` below for\n",
    "            more details, and possible values. By default, no pre-trained\n",
    "            weights are used.\n",
    "        progress (bool, optional): If True, displays a progress bar of the\n",
    "            download to stderr. Default is True.\n",
    "        **kwargs: parameters passed to the ``torchvision.models.resnet.ResNet``\n",
    "            base class. Please refer to the `source code\n",
    "            <https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py>`_\n",
    "            for more details about this class.\n",
    "\n",
    "    .. autoclass:: torchvision.models.ResNet50_Weights\n",
    "        :members:\n",
    "    \"\"\"\n",
    "    weights = ResNet50_Weights.verify(weights)\n",
    "\n",
    "    return _resnet(Bottleneck, [3, 4, 6, 3], weights, progress, **kwargs)\n",
    "\n",
    "\n",
    "\n",
    "@handle_legacy_interface(weights=(\"pretrained\", ResNet101_Weights.IMAGENET1K_V1))\n",
    "def resnet101(*, weights: Optional[ResNet101_Weights] = None, progress: bool = True, **kwargs: Any) -> ResNet:\n",
    "    \"\"\"ResNet-101 from `Deep Residual Learning for Image Recognition <https://arxiv.org/abs/1512.03385>`__.\n",
    "\n",
    "    .. note::\n",
    "       The bottleneck of TorchVision places the stride for downsampling to the second 3x3\n",
    "       convolution while the original paper places it to the first 1x1 convolution.\n",
    "       This variant improves the accuracy and is known as `ResNet V1.5\n",
    "       <https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch>`_.\n",
    "\n",
    "    Args:\n",
    "        weights (:class:`~torchvision.models.ResNet101_Weights`, optional): The\n",
    "            pretrained weights to use. See\n",
    "            :class:`~torchvision.models.ResNet101_Weights` below for\n",
    "            more details, and possible values. By default, no pre-trained\n",
    "            weights are used.\n",
    "        progress (bool, optional): If True, displays a progress bar of the\n",
    "            download to stderr. Default is True.\n",
    "        **kwargs: parameters passed to the ``torchvision.models.resnet.ResNet``\n",
    "            base class. Please refer to the `source code\n",
    "            <https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py>`_\n",
    "            for more details about this class.\n",
    "\n",
    "    .. autoclass:: torchvision.models.ResNet101_Weights\n",
    "        :members:\n",
    "    \"\"\"\n",
    "    weights = ResNet101_Weights.verify(weights)\n",
    "\n",
    "    return _resnet(Bottleneck, [3, 4, 23, 3], weights, progress, **kwargs)\n",
    "\n",
    "\n",
    "\n",
    "@handle_legacy_interface(weights=(\"pretrained\", ResNet152_Weights.IMAGENET1K_V1))\n",
    "def resnet152(*, weights: Optional[ResNet152_Weights] = None, progress: bool = True, **kwargs: Any) -> ResNet:\n",
    "    \"\"\"ResNet-152 from `Deep Residual Learning for Image Recognition <https://arxiv.org/abs/1512.03385>`__.\n",
    "\n",
    "    .. note::\n",
    "       The bottleneck of TorchVision places the stride for downsampling to the second 3x3\n",
    "       convolution while the original paper places it to the first 1x1 convolution.\n",
    "       This variant improves the accuracy and is known as `ResNet V1.5\n",
    "       <https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch>`_.\n",
    "\n",
    "    Args:\n",
    "        weights (:class:`~torchvision.models.ResNet152_Weights`, optional): The\n",
    "            pretrained weights to use. See\n",
    "            :class:`~torchvision.models.ResNet152_Weights` below for\n",
    "            more details, and possible values. By default, no pre-trained\n",
    "            weights are used.\n",
    "        progress (bool, optional): If True, displays a progress bar of the\n",
    "            download to stderr. Default is True.\n",
    "        **kwargs: parameters passed to the ``torchvision.models.resnet.ResNet``\n",
    "            base class. Please refer to the `source code\n",
    "            <https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py>`_\n",
    "            for more details about this class.\n",
    "\n",
    "    .. autoclass:: torchvision.models.ResNet152_Weights\n",
    "        :members:\n",
    "    \"\"\"\n",
    "    weights = ResNet152_Weights.verify(weights)\n",
    "\n",
    "    return _resnet(Bottleneck, [3, 8, 36, 3], weights, progress, **kwargs)\n",
    "\n",
    "\n",
    "\n",
    "@handle_legacy_interface(weights=(\"pretrained\", ResNeXt50_32X4D_Weights.IMAGENET1K_V1))\n",
    "def resnext50_32x4d(\n",
    "    *, weights: Optional[ResNeXt50_32X4D_Weights] = None, progress: bool = True, **kwargs: Any\n",
    ") -> ResNet:\n",
    "    \"\"\"ResNeXt-50 32x4d model from\n",
    "    `Aggregated Residual Transformation for Deep Neural Networks <https://arxiv.org/abs/1611.05431>`_.\n",
    "\n",
    "    Args:\n",
    "        weights (:class:`~torchvision.models.ResNeXt50_32X4D_Weights`, optional): The\n",
    "            pretrained weights to use. See\n",
    "            :class:`~torchvision.models.ResNext50_32X4D_Weights` below for\n",
    "            more details, and possible values. By default, no pre-trained\n",
    "            weights are used.\n",
    "        progress (bool, optional): If True, displays a progress bar of the\n",
    "            download to stderr. Default is True.\n",
    "        **kwargs: parameters passed to the ``torchvision.models.resnet.ResNet``\n",
    "            base class. Please refer to the `source code\n",
    "            <https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py>`_\n",
    "            for more details about this class.\n",
    "    .. autoclass:: torchvision.models.ResNeXt50_32X4D_Weights\n",
    "        :members:\n",
    "    \"\"\"\n",
    "    weights = ResNeXt50_32X4D_Weights.verify(weights)\n",
    "\n",
    "    _ovewrite_named_param(kwargs, \"groups\", 32)\n",
    "    _ovewrite_named_param(kwargs, \"width_per_group\", 4)\n",
    "    return _resnet(Bottleneck, [3, 4, 6, 3], weights, progress, **kwargs)\n",
    "\n",
    "\n",
    "\n",
    "@handle_legacy_interface(weights=(\"pretrained\", ResNeXt101_32X8D_Weights.IMAGENET1K_V1))\n",
    "def resnext101_32x8d(\n",
    "    *, weights: Optional[ResNeXt101_32X8D_Weights] = None, progress: bool = True, **kwargs: Any\n",
    ") -> ResNet:\n",
    "    \"\"\"ResNeXt-101 32x8d model from\n",
    "    `Aggregated Residual Transformation for Deep Neural Networks <https://arxiv.org/abs/1611.05431>`_.\n",
    "\n",
    "    Args:\n",
    "        weights (:class:`~torchvision.models.ResNeXt101_32X8D_Weights`, optional): The\n",
    "            pretrained weights to use. See\n",
    "            :class:`~torchvision.models.ResNeXt101_32X8D_Weights` below for\n",
    "            more details, and possible values. By default, no pre-trained\n",
    "            weights are used.\n",
    "        progress (bool, optional): If True, displays a progress bar of the\n",
    "            download to stderr. Default is True.\n",
    "        **kwargs: parameters passed to the ``torchvision.models.resnet.ResNet``\n",
    "            base class. Please refer to the `source code\n",
    "            <https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py>`_\n",
    "            for more details about this class.\n",
    "    .. autoclass:: torchvision.models.ResNeXt101_32X8D_Weights\n",
    "        :members:\n",
    "    \"\"\"\n",
    "    weights = ResNeXt101_32X8D_Weights.verify(weights)\n",
    "\n",
    "    _ovewrite_named_param(kwargs, \"groups\", 32)\n",
    "    _ovewrite_named_param(kwargs, \"width_per_group\", 8)\n",
    "    return _resnet(Bottleneck, [3, 4, 23, 3], weights, progress, **kwargs)\n",
    "\n",
    "\n",
    "\n",
    "@handle_legacy_interface(weights=(\"pretrained\", ResNeXt101_64X4D_Weights.IMAGENET1K_V1))\n",
    "def resnext101_64x4d(\n",
    "    *, weights: Optional[ResNeXt101_64X4D_Weights] = None, progress: bool = True, **kwargs: Any\n",
    ") -> ResNet:\n",
    "    \"\"\"ResNeXt-101 64x4d model from\n",
    "    `Aggregated Residual Transformation for Deep Neural Networks <https://arxiv.org/abs/1611.05431>`_.\n",
    "\n",
    "    Args:\n",
    "        weights (:class:`~torchvision.models.ResNeXt101_64X4D_Weights`, optional): The\n",
    "            pretrained weights to use. See\n",
    "            :class:`~torchvision.models.ResNeXt101_64X4D_Weights` below for\n",
    "            more details, and possible values. By default, no pre-trained\n",
    "            weights are used.\n",
    "        progress (bool, optional): If True, displays a progress bar of the\n",
    "            download to stderr. Default is True.\n",
    "        **kwargs: parameters passed to the ``torchvision.models.resnet.ResNet``\n",
    "            base class. Please refer to the `source code\n",
    "            <https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py>`_\n",
    "            for more details about this class.\n",
    "    .. autoclass:: torchvision.models.ResNeXt101_64X4D_Weights\n",
    "        :members:\n",
    "    \"\"\"\n",
    "    weights = ResNeXt101_64X4D_Weights.verify(weights)\n",
    "\n",
    "    _ovewrite_named_param(kwargs, \"groups\", 64)\n",
    "    _ovewrite_named_param(kwargs, \"width_per_group\", 4)\n",
    "    return _resnet(Bottleneck, [3, 4, 23, 3], weights, progress, **kwargs)\n",
    "\n",
    "\n",
    "\n",
    "@handle_legacy_interface(weights=(\"pretrained\", Wide_ResNet50_2_Weights.IMAGENET1K_V1))\n",
    "def wide_resnet50_2(\n",
    "    *, weights: Optional[Wide_ResNet50_2_Weights] = None, progress: bool = True, **kwargs: Any\n",
    ") -> ResNet:\n",
    "    \"\"\"Wide ResNet-50-2 model from\n",
    "    `Wide Residual Networks <https://arxiv.org/abs/1605.07146>`_.\n",
    "\n",
    "    The model is the same as ResNet except for the bottleneck number of channels\n",
    "    which is twice larger in every block. The number of channels in outer 1x1\n",
    "    convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048\n",
    "    channels, and in Wide ResNet-50-2 has 2048-1024-2048.\n",
    "\n",
    "    Args:\n",
    "        weights (:class:`~torchvision.models.Wide_ResNet50_2_Weights`, optional): The\n",
    "            pretrained weights to use. See\n",
    "            :class:`~torchvision.models.Wide_ResNet50_2_Weights` below for\n",
    "            more details, and possible values. By default, no pre-trained\n",
    "            weights are used.\n",
    "        progress (bool, optional): If True, displays a progress bar of the\n",
    "            download to stderr. Default is True.\n",
    "        **kwargs: parameters passed to the ``torchvision.models.resnet.ResNet``\n",
    "            base class. Please refer to the `source code\n",
    "            <https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py>`_\n",
    "            for more details about this class.\n",
    "    .. autoclass:: torchvision.models.Wide_ResNet50_2_Weights\n",
    "        :members:\n",
    "    \"\"\"\n",
    "    weights = Wide_ResNet50_2_Weights.verify(weights)\n",
    "\n",
    "    _ovewrite_named_param(kwargs, \"width_per_group\", 64 * 2)\n",
    "    return _resnet(Bottleneck, [3, 4, 6, 3], weights, progress, **kwargs)\n",
    "\n",
    "\n",
    "\n",
    "@handle_legacy_interface(weights=(\"pretrained\", Wide_ResNet101_2_Weights.IMAGENET1K_V1))\n",
    "def wide_resnet101_2(\n",
    "    *, weights: Optional[Wide_ResNet101_2_Weights] = None, progress: bool = True, **kwargs: Any\n",
    ") -> ResNet:\n",
    "    \"\"\"Wide ResNet-101-2 model from\n",
    "    `Wide Residual Networks <https://arxiv.org/abs/1605.07146>`_.\n",
    "\n",
    "    The model is the same as ResNet except for the bottleneck number of channels\n",
    "    which is twice larger in every block. The number of channels in outer 1x1\n",
    "    convolutions is the same, e.g. last block in ResNet-101 has 2048-512-2048\n",
    "    channels, and in Wide ResNet-101-2 has 2048-1024-2048.\n",
    "\n",
    "    Args:\n",
    "        weights (:class:`~torchvision.models.Wide_ResNet101_2_Weights`, optional): The\n",
    "            pretrained weights to use. See\n",
    "            :class:`~torchvision.models.Wide_ResNet101_2_Weights` below for\n",
    "            more details, and possible values. By default, no pre-trained\n",
    "            weights are used.\n",
    "        progress (bool, optional): If True, displays a progress bar of the\n",
    "            download to stderr. Default is True.\n",
    "        **kwargs: parameters passed to the ``torchvision.models.resnet.ResNet``\n",
    "            base class. Please refer to the `source code\n",
    "            <https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py>`_\n",
    "            for more details about this class.\n",
    "    .. autoclass:: torchvision.models.Wide_ResNet101_2_Weights\n",
    "        :members:\n",
    "    \"\"\"\n",
    "    weights = Wide_ResNet101_2_Weights.verify(weights)\n",
    "\n",
    "    _ovewrite_named_param(kwargs, \"width_per_group\", 64 * 2)\n",
    "    return _resnet(Bottleneck, [3, 4, 23, 3], weights, progress, **kwargs)\n",
    "\n",
    "\n",
    "\n",
    "def get_resnet_model(cuda: bool, model_derivative: str, pretrained: bool = True, model_state_dict: dict = None, **kwargs):\n",
    "    models = {\n",
    "        \"resnet18\": resnet18,\n",
    "        \"resnet34\": resnet34,\n",
    "        \"resnet50\": resnet50,\n",
    "        \"resnet101\": resnet101,\n",
    "        \"resnet152\": resnet152\n",
    "    }\n",
    "    assert model_derivative in models.keys(), \"Your selected resnet model derivative is unavailable\"\n",
    "\n",
    "    model = models[model_derivative](weights='ResNet50_Weights.DEFAULT', **kwargs) if pretrained else models[model_derivative](**kwargs)\n",
    "\n",
    "    if model_state_dict:\n",
    "        print(\"Loading pretrained model...\")\n",
    "        model.load_state_dict(model_state_dict)\n",
    "        print(\"Finished.\")\n",
    "    else:\n",
    "        print(\"Initializing parameters...\")\n",
    "        for para in model.parameters():\n",
    "            if para.dim() > 1:\n",
    "                nn.init.xavier_uniform_(para)\n",
    "        print(\"Finished.\")\n",
    "\n",
    "    if cuda:\n",
    "        model = model.to(\"cuda\")\n",
    "    return model\n"
   ],
   "metadata": {
    "id": "wJ22_w727nGp",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1708410797924,
     "user_tz": -420,
     "elapsed": 1402,
     "user": {
      "displayName": "Trong Nguyen Duc",
      "userId": "13500150630151633478"
     }
    }
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Utils"
   ],
   "metadata": {
    "id": "NckWwG517r0u"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "### Early stopping\n",
    "class EarlyStopper:\n",
    "    def __init__(self, PATIENCE=1, MIN_DELTA=0):\n",
    "        self.counter = 0\n",
    "        self.min_delta = MIN_DELTA\n",
    "        self.patience = PATIENCE  # Num of epoch that val loss is allowed to increase\n",
    "        self.min_val_loss = float('inf')\n",
    "\n",
    "    def early_stop(self, val_loss: float):\n",
    "        if val_loss < self.min_val_loss:\n",
    "            self.min_val_loss = val_loss\n",
    "            self.counter = 0\n",
    "        elif val_loss > (self.min_val_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "### Logger\n",
    "import json\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "class Logger:\n",
    "    __log_path: str\n",
    "    __checkpoint: bool\n",
    "\n",
    "    def __init__(self, log_path: str):\n",
    "        self.__log_path = log_path\n",
    "        self.__train_at = {\"Train at\": datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")}\n",
    "\n",
    "    def write(self, writing_mode: str = \"a\", **kwargs: dict):\n",
    "        # https://stackoverflow.com/questions/57727372/how-do-i-get-the-value-of-a-tensor-in-pytorch\n",
    "        with open(file=self.__log_path, mode=writing_mode, encoding=\"UTF-8\", errors=\"ignore\") as f:\n",
    "            # for k in kwargs.keys():\n",
    "            #     if isinstance(kwargs[k], torch.Tensor):\n",
    "            #         kwargs[k] = float(kwargs[k])\n",
    "\n",
    "            # kwargs = {print(v) for (k, v) in kwargs.items() if isinstance(v, tuple)}\n",
    "            # print(kwargs)\n",
    "\n",
    "            f.write(json.dumps(dict(self.__train_at, **kwargs), indent=4))\n",
    "            f.write(\"\\n\")\n",
    "### Utils\n",
    "import os\n",
    "\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "\n",
    "from torchsummary import summary\n",
    "from torchvision.transforms import v2\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import InterpolationMode\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "\n",
    "def get_train_set(root: str, input_size: int,\n",
    "                  train_size: float, batch_size: int,\n",
    "                  seed: int, cuda: bool, num_workers=1\n",
    "                  ) -> Tuple[DataLoader, DataLoader]:\n",
    "    # img -- reduce to 20% --> upsample to 224x224 -> Blur with ksize (10, 10)\n",
    "    # Make female samples equal male\n",
    "\n",
    "    # Use page-locked or not\n",
    "    pin_memory = True if cuda is True else False\n",
    "\n",
    "    train_set = ImageFolder(root=os.path.join(root, \"train\"),\n",
    "                            transform=v2.Compose([\n",
    "                                # img from celeb A: 178 x 218 x 3\n",
    "                                v2.Resize(size=(int(178 * .2), int(218 * .2)), interpolation=InterpolationMode.NEAREST),\n",
    "                                v2.Resize(size=(input_size, input_size), interpolation=InterpolationMode.BICUBIC),\n",
    "                                v2.GaussianBlur(kernel_size=5),\n",
    "                                v2.PILToTensor(),\n",
    "                                v2.ToDtype(torch.float32, scale=True),\n",
    "                                v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "                            ]))\n",
    "\n",
    "    train_set, validation_set = random_split(dataset=train_set,\n",
    "                                             generator=torch.Generator().manual_seed(seed),\n",
    "                                             lengths=[round(len(train_set) * train_size),\n",
    "                                                      len(train_set) - round(len(train_set) * train_size)\n",
    "                                                      ]\n",
    "                                             )\n",
    "\n",
    "    train_set = DataLoader(dataset=train_set,\n",
    "                           batch_size=batch_size,\n",
    "                           shuffle=True,\n",
    "                           num_workers=num_workers,\n",
    "                           pin_memory=pin_memory\n",
    "                           )\n",
    "\n",
    "    validation_set = DataLoader(dataset=validation_set,\n",
    "                                batch_size=batch_size,\n",
    "                                shuffle=True,\n",
    "                                num_workers=num_workers,\n",
    "                                pin_memory=pin_memory\n",
    "                                )\n",
    "    return train_set, validation_set\n",
    "\n",
    "\n",
    "def get_test_set(root: str, input_size: int, batch_size: int, seed: int, cuda: bool, num_workers=1) -> DataLoader:\n",
    "    # img -- reduce to 20% --> upsample to 224x224 -> Blur with ksize (10, 10)\n",
    "    # Make female samples equal male\n",
    "\n",
    "    # Use page-locked or not\n",
    "    pin_memory = True if cuda is True else False\n",
    "\n",
    "    test_set = ImageFolder(root=os.path.join(root, \"test\"),\n",
    "                           transform=v2.Compose([\n",
    "                               v2.Resize(size=(input_size, input_size), interpolation=InterpolationMode.BICUBIC),\n",
    "                               v2.PILToTensor(),\n",
    "                               v2.ToDtype(torch.float32, scale=True),\n",
    "                               v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "                           ]))\n",
    "\n",
    "    test_set = DataLoader(dataset=test_set,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=num_workers,\n",
    "                          pin_memory=pin_memory\n",
    "                          )\n",
    "    return test_set\n",
    "\n",
    "\n",
    "def get_model_summary(model: torch.nn.Module, input_size: Tuple):\n",
    "    return summary(model, input_size)\n"
   ],
   "metadata": {
    "id": "eHIDzLuw7sIu",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1708410797924,
     "user_tz": -420,
     "elapsed": 30,
     "user": {
      "displayName": "Trong Nguyen Duc",
      "userId": "13500150630151633478"
     }
    }
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Trainer"
   ],
   "metadata": {
    "id": "A4OAWgxE70zR"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "import torchvision.utils\n",
    "from box import Box\n",
    "from tqdm import tqdm\n",
    "from time import time, sleep\n",
    "from typing import List, Tuple\n",
    "# from src.utils.logger import Logger\n",
    "# from src.modelling.vgg import get_vgg_model\n",
    "# from src.modelling.resnet import get_resnet_model\n",
    "# from src.utils.early_stopping import EarlyStopper\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torcheval.metrics import MulticlassF1Score, BinaryF1Score, MulticlassAccuracy, BinaryAccuracy\n",
    "from torch.optim import (Adam, AdamW, NAdam, RAdam, SparseAdam, Adadelta, Adagrad, Adamax, ASGD, RMSprop, Rprop, LBFGS, SGD)\n",
    "from torch.nn.modules import (NLLLoss, NLLLoss2d, CTCLoss, KLDivLoss, GaussianNLLLoss, PoissonNLLLoss, L1Loss, MSELoss, HuberLoss, SmoothL1Loss, CrossEntropyLoss, BCELoss, BCEWithLogitsLoss)\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    __options: Box\n",
    "    __log_path: str\n",
    "    __checkpoint_path: str\n",
    "    __early_stopper: EarlyStopper\n",
    "    __train_loss: torch.nn.Module\n",
    "    __metrics: List\n",
    "    __start_epoch: int\n",
    "    __model: torch.nn.Module\n",
    "    __optimizer: torch.optim.Optimizer\n",
    "    __best_acc: float\n",
    "\n",
    "    def __init__(self, options: Box, log_path: str, checkpoint_path: str):\n",
    "        self.__options = options\n",
    "        self.__log_path = log_path\n",
    "        self.__checkpoint_path = checkpoint_path\n",
    "\n",
    "        self.__early_stopper = EarlyStopper(**self.__options.SOLVER.EARLY_STOPPING)\n",
    "        self.__train_loss = self.__init_loss()\n",
    "        self.__metrics = self.__init_metrics()\n",
    "        self.__start_epoch, self.__model, self.__optimizer = self.__init_model_optimizer_epoch()\n",
    "\n",
    "        if not self.__options.CHECKPOINT.SAVE_ALL:\n",
    "            self.__best_acc: float = self.__get_best_acc()\n",
    "\n",
    "    # Setter & Getter\n",
    "    @property\n",
    "    def model(self):\n",
    "        return self.__model\n",
    "\n",
    "    # Public methods\n",
    "    def train(self, train_set: DataLoader, validation_set: DataLoader, sleep_time: int = None) -> None:\n",
    "        print(\"Start training model ...\")\n",
    "        self.__model.train()\n",
    "\n",
    "        train_loss = None\n",
    "        logger = Logger(log_path=self.__log_path, )\n",
    "\n",
    "        # Start training\n",
    "        for epoch in range(self.__start_epoch, self.__start_epoch + self.__options.EPOCH.EPOCHS):\n",
    "            print(\"Epoch:\", epoch)\n",
    "            start_time = time()\n",
    "\n",
    "            for index, batch in tqdm(enumerate(train_set), total=len(train_set), colour=\"cyan\", desc=\"Training\"):\n",
    "                imgs, ground_truths = batch[0].type(torch.FloatTensor), batch[1].type(torch.FloatTensor)\n",
    "\n",
    "                # Pass to predefined device\n",
    "                if self.__options.MISC.CUDA:\n",
    "                    imgs = imgs.to(\"cuda\")\n",
    "                    ground_truths = ground_truths.to(\"cuda\")\n",
    "\n",
    "                # forward pass\n",
    "                if self.__options.SOLVER.MODEL.ARGS.num_classes == 1:\n",
    "                    predictions = torch.nn.functional.sigmoid(self.__model(imgs))\n",
    "                elif self.__options.SOLVER.MODEL.ARGS.num_classes > 1:\n",
    "                    predictions = torch.nn.functional.softmax(self.__model(imgs))\n",
    "\n",
    "                if predictions.shape[1] == 1:\n",
    "                    predictions = predictions.squeeze(1)\n",
    "\n",
    "                # Update loss\n",
    "                train_loss = self.__train_loss(predictions, ground_truths)\n",
    "\n",
    "                # Backprop\n",
    "                self.__optimizer.zero_grad()\n",
    "                train_loss.backward()\n",
    "\n",
    "                # Update weights\n",
    "                self.__optimizer.step()\n",
    "\n",
    "                # Update metrics\n",
    "                for metric in self.__metrics:\n",
    "                    metric.update(predictions, ground_truths)\n",
    "\n",
    "\n",
    "            # Training metrics\n",
    "            train_acc, train_f1 = [metric.compute().item() for metric in self.__metrics]\n",
    "\n",
    "            # Validate model\n",
    "            val_loss, val_acc, val_f1 = self.__validate(validation_set)\n",
    "\n",
    "            # Check early stopping cond\n",
    "            if self.__options.MISC.APPLY_EARLY_STOPPING:\n",
    "                if self.__early_stopper.early_stop(val_loss):\n",
    "                    break\n",
    "\n",
    "            # Logging\n",
    "            logger.write(writing_mode=\"a\",\n",
    "                         **{\"epoch\": epoch, \"time per epoch\": time() - start_time,\n",
    "                            \"train_loss\": train_loss.item(), \"train_acc\": train_acc, \"train_f1\": train_f1,\n",
    "                            \"val_loss\": val_loss, \"val_acc\": val_acc, \"val_f1\": val_f1\n",
    "                            }\n",
    "                         )\n",
    "\n",
    "            # Save checkpoint\n",
    "            if self.__options.CHECKPOINT.SAVE:\n",
    "                self.__save_checkpoint(epoch=epoch, train_acc=train_acc,\n",
    "                                       save_all=self.__options.CHECKPOINT.SAVE_ALL,\n",
    "                                       obj={\"epoch\": epoch, \"train_acc\": train_acc,\n",
    "                                            \"model_state_dict\": self.__model.state_dict(),\n",
    "                                            \"optimizer_state_dict\": self.__optimizer.state_dict()\n",
    "                                            }\n",
    "                                       )\n",
    "            # Reset metrics\n",
    "            for metric in self.__metrics:\n",
    "                metric.reset()\n",
    "\n",
    "            # Stop in short time\n",
    "            if sleep_time is not None:\n",
    "                sleep(sleep_time)\n",
    "        return None\n",
    "\n",
    "    # Private methods\n",
    "    def __validate(self, validation_set: DataLoader) -> List:\n",
    "        self.__model.eval()\n",
    "        val_loss = total_samples = 0\n",
    "        val_metrics = self.__init_metrics()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for index, batch in tqdm(enumerate(validation_set), total=len(validation_set), desc=\"Validating\"):\n",
    "                imgs, ground_truths = batch[0].type(torch.FloatTensor), batch[1].type(torch.FloatTensor)\n",
    "\n",
    "                if self.__options.MISC.CUDA:\n",
    "                    imgs = imgs.to(\"cuda\")\n",
    "                    ground_truths = ground_truths.to(\"cuda\")\n",
    "\n",
    "                # forward pass\n",
    "                if self.__options.SOLVER.MODEL.ARGS.num_classes == 1:\n",
    "                    predictions = torch.nn.functional.sigmoid(self.__model(imgs))\n",
    "                elif self.__options.SOLVER.MODEL.ARGS.num_classes > 1:\n",
    "                    predictions = torch.nn.functional.softmax(self.__model(imgs))\n",
    "\n",
    "                if predictions.shape[1] == 1:\n",
    "                    predictions = predictions.squeeze(1)\n",
    "\n",
    "                val_loss += self.__init_loss()(predictions, ground_truths) * imgs.size(0)\n",
    "                total_samples += imgs.size(0)\n",
    "\n",
    "                # update metrics\n",
    "                for metric in val_metrics:\n",
    "                    metric.update(predictions, ground_truths)\n",
    "\n",
    "        val_loss /= total_samples\n",
    "        return [val_loss.item()] + [metric.compute().item() for metric in val_metrics]\n",
    "\n",
    "    def __save_checkpoint(self, epoch: int, train_acc: float, obj: dict, save_all: bool = False) -> None:\n",
    "        \"\"\"\n",
    "        save_all:\n",
    "            True: save all trained epoch\n",
    "            False: save only last and the best trained epoch\n",
    "        \"\"\"\n",
    "        save_name = os.path.join(self.__checkpoint_path, f\"epoch_{epoch}.pt\")\n",
    "        torch.save(obj=obj, f=save_name)\n",
    "\n",
    "        if not save_all and epoch - 1 > 0:\n",
    "            # Remove previous epoch\n",
    "            os.remove(os.path.join(self.__checkpoint_path, f\"epoch_{epoch - 1}.pt\"))\n",
    "\n",
    "            # Save best checkpoint\n",
    "            if train_acc > self.__best_acc:\n",
    "                save_name = os.path.join(self.__checkpoint_path, f\"best_checkpoint.pt\")\n",
    "                torch.save(obj=obj, f=save_name)\n",
    "\n",
    "                # Update best accuracy\n",
    "                self.__best_acc = train_acc\n",
    "        return None\n",
    "\n",
    "    def __get_best_acc(self) -> float:\n",
    "        if \"best_checkpoint.pt\" in self.__checkpoint_path:\n",
    "            return torch.load(f=os.path.join(self.__checkpoint_path, \"best_checkpoint.pt\"))[\"train_acc\"]\n",
    "        else:\n",
    "            return 0.\n",
    "\n",
    "    def __init_model_optimizer_epoch(self) -> Tuple[int, torch.nn.Module, torch.optim.Optimizer]:\n",
    "        def init_optimizer(name: str, model_paras, state_dict=None, **kwargs) -> torch.optim.Optimizer:\n",
    "            available_optimizers = {\n",
    "                \"Adam\": Adam,\n",
    "                \"AdamW\": AdamW,\n",
    "                \"NAdam\": NAdam,\n",
    "                \"Adadelta\": Adadelta,\n",
    "                \"Adagrad\": Adagrad,\n",
    "                \"Adamax\": Adamax,\n",
    "                \"RAdam\": RAdam,\n",
    "                \"SparseAdam\": SparseAdam,\n",
    "                \"RMSprop\": RMSprop,\n",
    "                \"Rprop\": Rprop,\n",
    "                \"ASGD\": ASGD,\n",
    "                \"LBFGS\": LBFGS,\n",
    "                \"SGD\": SGD\n",
    "            }\n",
    "            assert name in available_optimizers.keys(), \"Your selected optimizer is unavailable.\"\n",
    "\n",
    "            # init optimizer\n",
    "            optimizer = available_optimizers[name](model_paras, **kwargs)\n",
    "\n",
    "            if state_dict is not None:\n",
    "                optimizer.load_state_dict(state_dict)\n",
    "            return optimizer\n",
    "\n",
    "        def init_model(cuda: bool, pretrained: bool, base: str, name: str, state_dict: dict, **kwargs) -> torch.nn.Module:\n",
    "            available_bases = {\n",
    "                \"vgg\": get_vgg_model,\n",
    "                \"resnet\": get_resnet_model\n",
    "            }\n",
    "            assert base in available_bases.keys(), \"Your selected base is unavailable\"\n",
    "            return available_bases[base](cuda, name, pretrained, state_dict, **kwargs)\n",
    "\n",
    "\n",
    "        model_state_dict = None\n",
    "        optimizer_state_dict = None\n",
    "        start_epoch = self.__options.EPOCH.START\n",
    "\n",
    "        if self.__options.CHECKPOINT.LOAD:\n",
    "            map_location = \"cuda\" if self.__options.MISC.CUDA else \"cpu\"\n",
    "            checkpoint = torch.load(f=os.path.join(self.__checkpoint_path, self.__options.CHECKPOINT.RESUME_NAME),\n",
    "                                    map_location=map_location)\n",
    "            start_epoch = checkpoint[\"epoch\"] + 1\n",
    "            model_state_dict = checkpoint[\"model_state_dict\"]\n",
    "            optimizer_state_dict = checkpoint[\"optimizer_state_dict\"]\n",
    "\n",
    "        model: torch.nn.Module = init_model(cuda=self.__options.MISC.CUDA,\n",
    "                                            pretrained=self.__options.SOLVER.MODEL.PRETRAINED,\n",
    "                                            base=self.__options.SOLVER.MODEL.BASE,\n",
    "                                            name=self.__options.SOLVER.MODEL.NAME,\n",
    "                                            state_dict=model_state_dict,\n",
    "                                            **self.__options.SOLVER.MODEL.ARGS)\n",
    "\n",
    "        optimizer: torch.optim.Optimizer = init_optimizer(name=self.__options.SOLVER.OPTIMIZER.NAME,\n",
    "                                                          model_paras=model.parameters(),\n",
    "                                                          state_dict=optimizer_state_dict,\n",
    "                                                          **self.__options.SOLVER.OPTIMIZER.ARGS)\n",
    "        return start_epoch, model, optimizer\n",
    "\n",
    "    def __init_metrics(self) -> List:\n",
    "        available_metrics = {\n",
    "            \"BinaryAccuracy\": BinaryAccuracy,\n",
    "            \"BinaryF1Score\": BinaryF1Score,\n",
    "\n",
    "            \"MulticlassAccuracy\": MulticlassAccuracy,\n",
    "            \"MulticlassF1Score\": MulticlassF1Score\n",
    "        }\n",
    "\n",
    "        # check whether metrics available or not\n",
    "        for metric in self.__options.METRICS.NAME_LIST:\n",
    "            assert metric in available_metrics.keys(), \"Your selected metric is unavailable\"\n",
    "\n",
    "        metrics = []\n",
    "        for i in range(len(self.__options.METRICS.NAME_LIST)):\n",
    "            metrics.append(available_metrics[self.__options.METRICS.NAME_LIST[i]](**self.__options.METRICS.ARGS[str(i)]))\n",
    "\n",
    "        if self.__options.MISC.CUDA:\n",
    "            metrics = [metric.to(\"cuda\") for metric in metrics]\n",
    "        return metrics\n",
    "\n",
    "\n",
    "    def __init_loss(self):\n",
    "        available_loss = {\n",
    "            \"NLLLoss\": NLLLoss, \"NLLLoss2d\": NLLLoss2d,\n",
    "            \"CTCLoss\": CTCLoss, \"KLDivLoss\": KLDivLoss,\n",
    "            \"GaussianNLLLoss\": GaussianNLLLoss, \"PoissonNLLLoss\": PoissonNLLLoss,\n",
    "            \"CrossEntropyLoss\": CrossEntropyLoss, \"BCELoss\": BCELoss, \"BCEWithLogitsLoss\": BCEWithLogitsLoss,\n",
    "            \"L1Loss\": L1Loss, \"MSELoss\": MSELoss, \"HuberLoss\": HuberLoss, \"SmoothL1Loss\": SmoothL1Loss,\n",
    "        }\n",
    "        assert self.__options.SOLVER.LOSS.NAME in available_loss.keys(), \"Your selected loss function is unavailable\"\n",
    "        return available_loss[self.__options.SOLVER.LOSS.NAME](** self.__options.SOLVER.LOSS.ARGS)\n"
   ],
   "metadata": {
    "id": "U5mkXzoA70_y",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1708410799693,
     "user_tz": -420,
     "elapsed": 1798,
     "user": {
      "displayName": "Trong Nguyen Duc",
      "userId": "13500150630151633478"
     }
    }
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Main"
   ],
   "metadata": {
    "id": "dVxXbHtl75xg"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "from box import Box\n",
    "# from src.tools.train import Trainer\n",
    "# from src.tools.eval import Evaluator\n",
    "# from src.tools.visualization import training_visualization\n",
    "# from src.utils.utils import get_train_set, get_test_set, get_model_summary\n",
    "\n",
    "\n",
    "def train() -> None:\n",
    "    # Load dataset\n",
    "    global options\n",
    "    log_path = os.path.join(\"/content/drive/MyDrive/face_attribute/logs\", f\"{options.SOLVER.MODEL.NAME}_training_log.json\")\n",
    "    checkpoint_path = os.path.join(\"/content/drive/MyDrive/face_attribute/checkpoints\", options.SOLVER.MODEL.NAME)\n",
    "\n",
    "    if not os.path.isdir(checkpoint_path):\n",
    "        os.mkdir(path=checkpoint_path, mode=0x777)\n",
    "        print(f\"directory checkpoint for {options.SOLVER.MODEL.NAME} is created.\")\n",
    "\n",
    "    train_set, validation_set = get_train_set(root=os.path.join(os.getcwd(), options.DATA.DATASET_NAME),\n",
    "                                              input_size=options.DATA.INPUT_SHAPE[0],\n",
    "                                              train_size=options.DATA.TRAIN_SIZE,\n",
    "                                              batch_size=options.DATA.BATCH_SIZE,\n",
    "                                              seed=options.MISC.SEED, cuda=options.MISC.CUDA,\n",
    "                                              num_workers=options.DATA.NUM_WORKERS)\n",
    "    print(f\"\"\"Train batch: {len(train_set)}, Validation batch: {len(validation_set)}\"\"\")\n",
    "\n",
    "    trainer = Trainer(options=options, log_path=log_path, checkpoint_path=checkpoint_path)\n",
    "    trainer.train(train_set, validation_set)\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    train()\n",
    "    return None\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ],
   "metadata": {
    "id": "PHlviZaf752B",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "41f5ee8c-2679-4db4-c062-f7fad7320455"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train batch: 347, Validation batch: 39\n",
      "Initializing parameters...\n",
      "Finished.\n",
      "Start training model ...\n",
      "Epoch: 1\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training: 100%|\u001B[36m██████████\u001B[0m| 347/347 [05:55<00:00,  1.02s/it]\n",
      "Validating: 100%|██████████| 39/39 [00:36<00:00,  1.06it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 2\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training: 100%|\u001B[36m██████████\u001B[0m| 347/347 [06:48<00:00,  1.18s/it]\n",
      "Validating: 100%|██████████| 39/39 [00:37<00:00,  1.05it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 3\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training: 100%|\u001B[36m██████████\u001B[0m| 347/347 [06:47<00:00,  1.17s/it]\n",
      "Validating: 100%|██████████| 39/39 [00:37<00:00,  1.05it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 4\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training: 100%|\u001B[36m██████████\u001B[0m| 347/347 [06:46<00:00,  1.17s/it]\n",
      "Validating: 100%|██████████| 39/39 [00:37<00:00,  1.05it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 5\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training: 100%|\u001B[36m██████████\u001B[0m| 347/347 [06:47<00:00,  1.17s/it]\n",
      "Validating: 100%|██████████| 39/39 [00:38<00:00,  1.02it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 6\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training: 100%|\u001B[36m██████████\u001B[0m| 347/347 [06:47<00:00,  1.17s/it]\n",
      "Validating: 100%|██████████| 39/39 [00:37<00:00,  1.03it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 7\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training: 100%|\u001B[36m██████████\u001B[0m| 347/347 [06:47<00:00,  1.17s/it]\n",
      "Validating: 100%|██████████| 39/39 [00:38<00:00,  1.03it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 8\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training: 100%|\u001B[36m██████████\u001B[0m| 347/347 [06:47<00:00,  1.17s/it]\n",
      "Validating: 100%|██████████| 39/39 [00:37<00:00,  1.03it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 9\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training: 100%|\u001B[36m██████████\u001B[0m| 347/347 [06:47<00:00,  1.18s/it]\n",
      "Validating: 100%|██████████| 39/39 [00:37<00:00,  1.05it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 10\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training: 100%|\u001B[36m██████████\u001B[0m| 347/347 [06:47<00:00,  1.17s/it]\n",
      "Validating: 100%|██████████| 39/39 [00:37<00:00,  1.03it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 11\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training: 100%|\u001B[36m██████████\u001B[0m| 347/347 [06:47<00:00,  1.17s/it]\n",
      "Validating: 100%|██████████| 39/39 [00:38<00:00,  1.02it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 12\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training: 100%|\u001B[36m██████████\u001B[0m| 347/347 [06:47<00:00,  1.17s/it]\n",
      "Validating: 100%|██████████| 39/39 [00:37<00:00,  1.05it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 13\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training:  92%|\u001B[36m█████████▏\u001B[0m| 318/347 [06:14<00:33,  1.17s/it]"
     ]
    }
   ]
  }
 ]
}
