{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7751531,"sourceType":"datasetVersion","datasetId":4532143},{"sourceId":7780664,"sourceType":"datasetVersion","datasetId":4553100}],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":358.352324,"end_time":"2024-02-19T07:20:52.425379","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-02-19T07:14:54.073055","version":"2.5.0"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Install packages","metadata":{"_cell_guid":"4dc0390e-270b-4812-b7ac-b671540ea55e","_uuid":"e24211f7-03d6-4bca-8985-044be03d0237","papermill":{"duration":0.006583,"end_time":"2024-02-19T07:14:56.865023","exception":false,"start_time":"2024-02-19T07:14:56.858440","status":"completed"},"tags":[]}},{"cell_type":"code","source":"!pip install torchsummary commentjson torcheval python-box[all]~=7.0","metadata":{"_cell_guid":"4eebf213-2a10-4727-8a24-26fbc2111346","_uuid":"d0137aeb-0c0e-4382-b9f0-d62e890cf5a6","collapsed":false,"jupyter":{"outputs_hidden":false},"papermill":{"duration":14.321141,"end_time":"2024-02-19T07:15:11.192429","exception":false,"start_time":"2024-02-19T07:14:56.871288","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training configs","metadata":{"papermill":{"duration":0.007181,"end_time":"2024-02-19T07:15:11.207242","exception":false,"start_time":"2024-02-19T07:15:11.200061","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from box import Box\n\noptions ={\n    \"EXPERIMENTS\":\n    {\n        \"PROJECT_NAME\": \"face_attribute\"\n    },\n\n    \"DATA\":\n    {\n        \"DATASET_NAME\": \"celeb_A\",\n        \"INPUT_SHAPE\": [3, 224, 224],\n        \"TRAIN_SIZE\": 0.9,\n        \"BATCH_SIZE\": 360,\n        \"NUM_WORKERS\": 4,\n        \"TRANSFORM\":\n        {\n            \"NAME_LIST\": [\"PILToTensor\", \"ToDtype\", \"RandomRotation\"],\n            \"ARGS\":\n            {\n                \"0\": {},\n\n                \"1\":\n                {\n                    \"dtype\": \"float32\",\n                    # normalize image from range [0, 255] into range [0, 1]\n                    \"scale\": True\n                },\n\n                \"2\":\n                {\n                    \"degrees\": [-10, 10],\n                    \"interpolation\": \"NEAREST\"\n                }\n            }\n        }\n    },\n\n    \"CHECKPOINT\":\n    {\n        \"SAVE\": True,\n        \"LOAD\": True,\n        \"SAVE_ALL\": False,\n        \"RESUME_NAME\": \"epoch_30.pt\"\n    },\n\n    \"EPOCH\":\n    {\n        \"START\": 1,\n        \"EPOCHS\": 1\n    },\n\n    \"METRICS\":\n    {\n        \"NAME_LIST\": [\"BinaryAccuracy\", \"BinaryF1Score\"],\n        \"ARGS\":\n        {\n            \"0\":\n            {\n               \"threshold\": 0.5\n            },\n\n            \"1\":\n            {\n               \"threshold\": 0.5\n            }\n        }\n    },\n\n    \"SOLVER\":\n    {\n        \"MODEL\":\n        {\n            \"BASE\": \"vgg\",\n            \"NAME\": \"vgg13\",\n            \"PRETRAINED\": False,\n            \"ARGS\":\n            {\n                \"num_classes\": 1\n            }\n        },\n\n        \"OPTIMIZER\":\n        {\n            \"NAME\": \"Adam\",\n            \"ARGS\":\n            {\n                \"lr\": 1e-7,\n                \"amsgrad\": True\n            }\n        },\n\n        \"LR_SCHEDULER\":\n        {\n            \"NAME\": \"CosineAnnealingWarmRestarts\",\n            \"ARGS\":\n            {\n                \"T_0\": 100,\n                \"T_mult\": 3,\n            }\n        },\n\n        \"LOSS\":\n        {\n            \"NAME\": \"BCELoss\",\n            \"ARGS\":\n            {\n                \"reduction\": \"mean\"\n            }\n        },\n\n        \"EARLY_STOPPING\":\n        {\n            \"PATIENCE\": 5,\n            \"MIN_DELTA\": 0\n        }\n    },\n\n    \"MISC\":\n    {\n        \"SEED\": 12345,\n        \"APPLY_EARLY_STOPPING\": True,\n        \"CUDA\": True\n    }\n}\noptions = Box(options)","metadata":{"papermill":{"duration":0.121176,"end_time":"2024-02-19T07:15:11.335543","exception":false,"start_time":"2024-02-19T07:15:11.214367","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Copy existing checkpoint for continuous training","metadata":{}},{"cell_type":"code","source":"if options.CHECKPOINT.LOAD:\n    !cp -r /kaggle/input/checkpoints /kaggle/working","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Vgg base","metadata":{"_cell_guid":"fe22d5e5-20ef-4a77-8428-4926489609ff","_uuid":"03c50d6d-817d-473b-9ba6-464553be3c96","execution":{"iopub.execute_input":"2024-02-06T13:47:25.496365Z","iopub.status.busy":"2024-02-06T13:47:25.495560Z","iopub.status.idle":"2024-02-06T13:49:01.208757Z","shell.execute_reply":"2024-02-06T13:49:01.206403Z","shell.execute_reply.started":"2024-02-06T13:47:25.496329Z"},"jupyter":{"outputs_hidden":true},"papermill":{"duration":0.008573,"end_time":"2024-02-19T07:15:11.354814","exception":false,"start_time":"2024-02-19T07:15:11.346241","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.utils.model_zoo as model_zoo\n\n__all__ = [\n    'VGG', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn',\n    'vgg19_bn', 'vgg19', \"get_vgg_model\"\n]\n\n\nmodel_urls = {\n    'vgg11': 'https://download.pytorch.org/models/vgg11-bbd30ac9.pth',\n    'vgg13': 'https://download.pytorch.org/models/vgg13-c768596a.pth',\n    'vgg16': 'https://download.pytorch.org/models/vgg16-397923af.pth',\n    'vgg19': 'https://download.pytorch.org/models/vgg19-dcbb9e9d.pth',\n    'vgg11_bn': 'https://download.pytorch.org/models/vgg11_bn-6002323d.pth',\n    'vgg13_bn': 'https://download.pytorch.org/models/vgg13_bn-abd245e5.pth',\n    'vgg16_bn': 'https://download.pytorch.org/models/vgg16_bn-6c64b313.pth',\n    'vgg19_bn': 'https://download.pytorch.org/models/vgg19_bn-c79401a0.pth',\n}\n\ncfg = {\n    'A': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n    'B': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n    'D': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n    'E': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n}\n\n\nclass VGG(nn.Module):\n    def __init__(self, features, dropout=.5, num_classes=1000, init_weights=True):\n        super(VGG, self).__init__()\n        self.features = features\n        self.classifier = nn.Sequential(\n            nn.Linear(512 * 7 * 7, 4096),\n            nn.ReLU(True),\n            nn.Dropout(dropout),\n\n            nn.Linear(4096, 4096),\n            nn.ReLU(True),\n            nn.Dropout(dropout),\n\n            nn.Linear(4096, 2048),\n            nn.ReLU(True),\n            nn.Dropout(dropout),\n\n            nn.Linear(2048, 1024),\n            nn.ReLU(True),\n            nn.Dropout(dropout),\n\n            nn.Linear(1024, 512),\n            nn.ReLU(True),\n            nn.Dropout(dropout),\n\n            nn.Linear(512, num_classes),\n            nn.Softmax(dim=1)\n        )\n        if init_weights:\n            self._initialize_weights()\n\n    def forward(self, x):\n        x = self.features(x)\n        x = x.view(x.size(0), -1)\n        x = self.classifier(x)\n        return x\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(\n                    m.weight, mode='fan_out', nonlinearity='relu')\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, 0, 0.01)\n                nn.init.constant_(m.bias, 0)\n\n\ndef make_layers(cfg, batch_norm=False):\n    layers = []\n    in_channels = 3\n    for v in cfg:\n        if v == 'M':\n            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n        else:\n            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n            if batch_norm:\n                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n            else:\n                layers += [conv2d, nn.ReLU(inplace=True)]\n            in_channels = v\n    return nn.Sequential(*layers)\n\n\n\n\ndef vgg11(pretrained=False, **kwargs):\n    \"\"\"VGG 11-layer model (configuration \"A\")\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    \"\"\"\n    if pretrained:\n        kwargs['init_weights'] = False\n    model = VGG(make_layers(cfg['A']), **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls['vgg11']))\n    return model\n\n\ndef vgg11_bn(pretrained=False, **kwargs):\n    \"\"\"VGG 11-layer model (configuration \"A\") with batch normalization\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    \"\"\"\n    if pretrained:\n        kwargs['init_weights'] = False\n    model = VGG(make_layers(cfg['A'], batch_norm=True), **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls['vgg11_bn']))\n    return model\n\n\ndef vgg13(pretrained=False, **kwargs):\n    \"\"\"VGG 13-layer model (configuration \"B\")\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    \"\"\"\n    if pretrained:\n        kwargs['init_weights'] = False\n    model = VGG(make_layers(cfg['B']), **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls['vgg13']))\n    return model\n\n\ndef vgg13_bn(pretrained=False, **kwargs):\n    \"\"\"VGG 13-layer model (configuration \"B\") with batch normalization\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    \"\"\"\n    if pretrained:\n        kwargs['init_weights'] = False\n    model = VGG(make_layers(cfg['B'], batch_norm=True), **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls['vgg13_bn']))\n    return model\n\n\ndef vgg16(pretrained=False, **kwargs):\n    \"\"\"VGG 16-layer model (configuration \"D\")\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    \"\"\"\n    if pretrained:\n        kwargs['init_weights'] = False\n    model = VGG(make_layers(cfg['D']), **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls['vgg16']))\n    return model\n\n\ndef vgg16_bn(pretrained=False, **kwargs):\n    \"\"\"VGG 16-layer model (configuration \"D\") with batch normalization\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    \"\"\"\n    if pretrained:\n        kwargs['init_weights'] = False\n    model = VGG(make_layers(cfg['D'], batch_norm=True), **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls['vgg16_bn']))\n    return model\n\n\ndef vgg19(pretrained=False, **kwargs):\n    \"\"\"VGG 19-layer model (configuration \"E\")\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    \"\"\"\n    if pretrained:\n        kwargs['init_weights'] = False\n    model = VGG(make_layers(cfg['E']), **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls['vgg19']))\n    return model\n\n\ndef vgg19_bn(pretrained=False, **kwargs):\n    \"\"\"VGG 19-layer model (configuration 'E') with batch normalization\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    \"\"\"\n    if pretrained:\n        kwargs['init_weights'] = False\n    model = VGG(make_layers(cfg['E'], batch_norm=True), **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls['vgg19_bn']))\n    return model\n\n\ndef get_vgg_model(cuda: bool, name: str, pretrained: bool = True, model_state_dict: dict = None, **kwargs):\n    models = {\n        \"vgg13\": vgg13,\n        \"vgg16\": vgg16,\n        \"vgg19\": vgg19,\n        \"vgg13_bn\": vgg13_bn,\n        \"vgg16_bn\": vgg16_bn,\n        \"vgg19_bn\": vgg19_bn,\n    }\n    assert name in models.keys(), \"Your selected vgg model derivative is unavailable\"\n\n    model = models[name](pretrained=pretrained, **kwargs)\n\n    if model_state_dict:\n        print(\"Loading pretrained model...\")\n        model.load_state_dict(model_state_dict)\n        print(\"Finished.\")\n    else:\n        print(\"Initializing parameters...\")\n        for para in model.parameters():\n            if para.dim() > 1:\n                nn.init.xavier_uniform_(para)\n        print(\"Finished.\")\n\n    if cuda:\n        model = model.to(\"cuda\")\n    return model\n","metadata":{"_cell_guid":"438fe9c8-5f51-4a5c-94ea-cd7a3b276145","_uuid":"de03e398-02e1-433d-b32c-9a22d3fa7377","collapsed":false,"jupyter":{"outputs_hidden":false},"papermill":{"duration":3.78308,"end_time":"2024-02-19T07:15:15.144863","exception":false,"start_time":"2024-02-19T07:15:11.361783","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Restnet base","metadata":{"_cell_guid":"d849a060-f776-4d5b-8e04-cd74c1d411ba","_uuid":"d93df6c0-7d6a-4ac5-af3b-1a5fb8a82002","papermill":{"duration":0.006972,"end_time":"2024-02-19T07:15:15.159195","exception":false,"start_time":"2024-02-19T07:15:15.152223","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from functools import partial\nfrom typing import Any, Callable, List, Optional, Type, Union\n\nimport torch\nimport torch.nn as nn\nfrom torch import Tensor\n\nfrom torchvision.transforms._presets import ImageClassification\n\nfrom torchvision.utils import _log_api_usage_once\nfrom torchvision.models._api import register_model, Weights, WeightsEnum\nfrom torchvision.models._utils import _ovewrite_named_param, handle_legacy_interface\nfrom torchvision.models._meta import _IMAGENET_CATEGORIES\n\n\n__all__ = [\n    \"ResNet\",\n    \"ResNet18_Weights\",\n    \"ResNet34_Weights\",\n    \"ResNet50_Weights\",\n    \"ResNet101_Weights\",\n    \"ResNet152_Weights\",\n    \"ResNeXt50_32X4D_Weights\",\n    \"ResNeXt101_32X8D_Weights\",\n    \"ResNeXt101_64X4D_Weights\",\n    \"Wide_ResNet50_2_Weights\",\n    \"Wide_ResNet101_2_Weights\",\n    \"resnet18\",\n    \"resnet34\",\n    \"resnet50\",\n    \"resnet101\",\n    \"resnet152\",\n    \"resnext50_32x4d\",\n    \"resnext101_32x8d\",\n    \"resnext101_64x4d\",\n    \"wide_resnet50_2\",\n    \"wide_resnet101_2\",\n]\n\n\ndef conv3x3(in_planes: int, out_planes: int, stride: int = 1, groups: int = 1, dilation: int = 1) -> nn.Conv2d:\n    \"\"\"3x3 convolution with padding\"\"\"\n    return nn.Conv2d(\n        in_planes,\n        out_planes,\n        kernel_size=3,\n        stride=stride,\n        padding=dilation,\n        groups=groups,\n        bias=False,\n        dilation=dilation,\n    )\n\n\ndef conv1x1(in_planes: int, out_planes: int, stride: int = 1) -> nn.Conv2d:\n    \"\"\"1x1 convolution\"\"\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion: int = 1\n\n    def __init__(\n        self,\n        inplanes: int,\n        planes: int,\n        stride: int = 1,\n        downsample: Optional[nn.Module] = None,\n        groups: int = 1,\n        base_width: int = 64,\n        dilation: int = 1,\n        norm_layer: Optional[Callable[..., nn.Module]] = None,\n    ) -> None:\n        super().__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        if groups != 1 or base_width != 64:\n            raise ValueError(\"BasicBlock only supports groups=1 and base_width=64\")\n        if dilation > 1:\n            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = norm_layer(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = norm_layer(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x: Tensor) -> Tensor:\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\n    # according to \"Deep residual learning for image recognition\" https://arxiv.org/abs/1512.03385.\n    # This variant is also known as ResNet V1.5 and improves accuracy according to\n    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.\n\n    expansion: int = 4\n\n    def __init__(\n        self,\n        inplanes: int,\n        planes: int,\n        stride: int = 1,\n        downsample: Optional[nn.Module] = None,\n        groups: int = 1,\n        base_width: int = 64,\n        dilation: int = 1,\n        norm_layer: Optional[Callable[..., nn.Module]] = None,\n    ) -> None:\n        super().__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        width = int(planes * (base_width / 64.0)) * groups\n        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n        self.conv1 = conv1x1(inplanes, width)\n        self.bn1 = norm_layer(width)\n        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n        self.bn2 = norm_layer(width)\n        self.conv3 = conv1x1(width, planes * self.expansion)\n        self.bn3 = norm_layer(planes * self.expansion)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x: Tensor) -> Tensor:\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNet(nn.Module):\n    def __init__(\n        self,\n        block: Type[Union[BasicBlock, Bottleneck]],\n        layers: List[int],\n        num_classes: int = 1000,\n        zero_init_residual: bool = False,\n        groups: int = 1,\n        width_per_group: int = 64,\n        replace_stride_with_dilation: Optional[List[bool]] = None,\n        norm_layer: Optional[Callable[..., nn.Module]] = None,\n    ) -> None:\n        super().__init__()\n        _log_api_usage_once(self)\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        self._norm_layer = norm_layer\n\n        self.inplanes = 64\n        self.dilation = 1\n        if replace_stride_with_dilation is None:\n            # each element in the tuple indicates if we should replace\n            # the 2x2 stride with a dilated convolution instead\n            replace_stride_with_dilation = [False, False, False]\n        if len(replace_stride_with_dilation) != 3:\n            raise ValueError(\n                \"replace_stride_with_dilation should be None \"\n                f\"or a 3-element tuple, got {replace_stride_with_dilation}\"\n            )\n        self.groups = groups\n        self.base_width = width_per_group\n        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False)\n        self.bn1 = norm_layer(self.inplanes)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, dilate=replace_stride_with_dilation[0])\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2, dilate=replace_stride_with_dilation[1])\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2, dilate=replace_stride_with_dilation[2])\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n        # Zero-initialize the last BN in each residual branch,\n        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n        if zero_init_residual:\n            for m in self.modules():\n                if isinstance(m, Bottleneck) and m.bn3.weight is not None:\n                    nn.init.constant_(m.bn3.weight, 0)  # type: ignore[arg-type]\n                elif isinstance(m, BasicBlock) and m.bn2.weight is not None:\n                    nn.init.constant_(m.bn2.weight, 0)  # type: ignore[arg-type]\n\n    def _make_layer(\n        self,\n        block: Type[Union[BasicBlock, Bottleneck]],\n        planes: int,\n        blocks: int,\n        stride: int = 1,\n        dilate: bool = False,\n    ) -> nn.Sequential:\n        norm_layer = self._norm_layer\n        downsample = None\n        previous_dilation = self.dilation\n        if dilate:\n            self.dilation *= stride\n            stride = 1\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                conv1x1(self.inplanes, planes * block.expansion, stride),\n                norm_layer(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(\n            block(\n                self.inplanes, planes, stride, downsample, self.groups, self.base_width, previous_dilation, norm_layer\n            )\n        )\n        self.inplanes = planes * block.expansion\n        for _ in range(1, blocks):\n            layers.append(\n                block(\n                    self.inplanes,\n                    planes,\n                    groups=self.groups,\n                    base_width=self.base_width,\n                    dilation=self.dilation,\n                    norm_layer=norm_layer,\n                )\n            )\n\n        return nn.Sequential(*layers)\n\n    def _forward_impl(self, x: Tensor) -> Tensor:\n        # See note [TorchScript super()]\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.fc(x)\n\n        return x\n\n    def forward(self, x: Tensor) -> Tensor:\n        return self._forward_impl(x)\n\n\ndef _resnet(\n    block: Type[Union[BasicBlock, Bottleneck]],\n    layers: List[int],\n    weights: Optional[WeightsEnum],\n    progress: bool,\n    **kwargs: Any,\n) -> ResNet:\n    if weights is not None:\n        _ovewrite_named_param(kwargs, \"num_classes\", len(weights.meta[\"categories\"]))\n\n    model = ResNet(block, layers, **kwargs)\n\n    if weights is not None:\n        model.load_state_dict(weights.get_state_dict(progress=progress, check_hash=True))\n\n    return model\n\n\n_COMMON_META = {\n    \"min_size\": (1, 1),\n    \"categories\": _IMAGENET_CATEGORIES,\n}\n\n\nclass ResNet18_Weights(WeightsEnum):\n    IMAGENET1K_V1 = Weights(\n        url=\"https://download.pytorch.org/models/resnet18-f37072fd.pth\",\n        transforms=partial(ImageClassification, crop_size=224),\n        meta={\n            **_COMMON_META,\n            \"num_params\": 11689512,\n            \"recipe\": \"https://github.com/pytorch/vision/tree/main/references/classification#resnet\",\n            \"_metrics\": {\n                \"ImageNet-1K\": {\n                    \"acc@1\": 69.758,\n                    \"acc@5\": 89.078,\n                }\n            },\n            \"_ops\": 1.814,\n            \"_file_size\": 44.661,\n            \"_docs\": \"\"\"These weights reproduce closely the results of the paper using a simple training recipe.\"\"\",\n        },\n    )\n    DEFAULT = IMAGENET1K_V1\n\n\nclass ResNet34_Weights(WeightsEnum):\n    IMAGENET1K_V1 = Weights(\n        url=\"https://download.pytorch.org/models/resnet34-b627a593.pth\",\n        transforms=partial(ImageClassification, crop_size=224),\n        meta={\n            **_COMMON_META,\n            \"num_params\": 21797672,\n            \"recipe\": \"https://github.com/pytorch/vision/tree/main/references/classification#resnet\",\n            \"_metrics\": {\n                \"ImageNet-1K\": {\n                    \"acc@1\": 73.314,\n                    \"acc@5\": 91.420,\n                }\n            },\n            \"_ops\": 3.664,\n            \"_file_size\": 83.275,\n            \"_docs\": \"\"\"These weights reproduce closely the results of the paper using a simple training recipe.\"\"\",\n        },\n    )\n    DEFAULT = IMAGENET1K_V1\n\n\nclass ResNet50_Weights(WeightsEnum):\n    IMAGENET1K_V1 = Weights(\n        url=\"https://download.pytorch.org/models/resnet50-0676ba61.pth\",\n        transforms=partial(ImageClassification, crop_size=224),\n        meta={\n            **_COMMON_META,\n            \"num_params\": 25557032,\n            \"recipe\": \"https://github.com/pytorch/vision/tree/main/references/classification#resnet\",\n            \"_metrics\": {\n                \"ImageNet-1K\": {\n                    \"acc@1\": 76.130,\n                    \"acc@5\": 92.862,\n                }\n            },\n            \"_ops\": 4.089,\n            \"_file_size\": 97.781,\n            \"_docs\": \"\"\"These weights reproduce closely the results of the paper using a simple training recipe.\"\"\",\n        },\n    )\n    IMAGENET1K_V2 = Weights(\n        url=\"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\",\n        transforms=partial(ImageClassification, crop_size=224, resize_size=232),\n        meta={\n            **_COMMON_META,\n            \"num_params\": 25557032,\n            \"recipe\": \"https://github.com/pytorch/vision/issues/3995#issuecomment-1013906621\",\n            \"_metrics\": {\n                \"ImageNet-1K\": {\n                    \"acc@1\": 80.858,\n                    \"acc@5\": 95.434,\n                }\n            },\n            \"_ops\": 4.089,\n            \"_file_size\": 97.79,\n            \"_docs\": \"\"\"\n                These weights improve upon the results of the original paper by using TorchVision's `new training recipe\n                <https://pytorch.org/blog/how-to-train-state-of-the-art-models-using-torchvision-latest-primitives/>`_.\n            \"\"\",\n        },\n    )\n    DEFAULT = IMAGENET1K_V2\n\n\nclass ResNet101_Weights(WeightsEnum):\n    IMAGENET1K_V1 = Weights(\n        url=\"https://download.pytorch.org/models/resnet101-63fe2227.pth\",\n        transforms=partial(ImageClassification, crop_size=224),\n        meta={\n            **_COMMON_META,\n            \"num_params\": 44549160,\n            \"recipe\": \"https://github.com/pytorch/vision/tree/main/references/classification#resnet\",\n            \"_metrics\": {\n                \"ImageNet-1K\": {\n                    \"acc@1\": 77.374,\n                    \"acc@5\": 93.546,\n                }\n            },\n            \"_ops\": 7.801,\n            \"_file_size\": 170.511,\n            \"_docs\": \"\"\"These weights reproduce closely the results of the paper using a simple training recipe.\"\"\",\n        },\n    )\n    IMAGENET1K_V2 = Weights(\n        url=\"https://download.pytorch.org/models/resnet101-cd907fc2.pth\",\n        transforms=partial(ImageClassification, crop_size=224, resize_size=232),\n        meta={\n            **_COMMON_META,\n            \"num_params\": 44549160,\n            \"recipe\": \"https://github.com/pytorch/vision/issues/3995#new-recipe\",\n            \"_metrics\": {\n                \"ImageNet-1K\": {\n                    \"acc@1\": 81.886,\n                    \"acc@5\": 95.780,\n                }\n            },\n            \"_ops\": 7.801,\n            \"_file_size\": 170.53,\n            \"_docs\": \"\"\"\n                These weights improve upon the results of the original paper by using TorchVision's `new training recipe\n                <https://pytorch.org/blog/how-to-train-state-of-the-art-models-using-torchvision-latest-primitives/>`_.\n            \"\"\",\n        },\n    )\n    DEFAULT = IMAGENET1K_V2\n\n\nclass ResNet152_Weights(WeightsEnum):\n    IMAGENET1K_V1 = Weights(\n        url=\"https://download.pytorch.org/models/resnet152-394f9c45.pth\",\n        transforms=partial(ImageClassification, crop_size=224),\n        meta={\n            **_COMMON_META,\n            \"num_params\": 60192808,\n            \"recipe\": \"https://github.com/pytorch/vision/tree/main/references/classification#resnet\",\n            \"_metrics\": {\n                \"ImageNet-1K\": {\n                    \"acc@1\": 78.312,\n                    \"acc@5\": 94.046,\n                }\n            },\n            \"_ops\": 11.514,\n            \"_file_size\": 230.434,\n            \"_docs\": \"\"\"These weights reproduce closely the results of the paper using a simple training recipe.\"\"\",\n        },\n    )\n    IMAGENET1K_V2 = Weights(\n        url=\"https://download.pytorch.org/models/resnet152-f82ba261.pth\",\n        transforms=partial(ImageClassification, crop_size=224, resize_size=232),\n        meta={\n            **_COMMON_META,\n            \"num_params\": 60192808,\n            \"recipe\": \"https://github.com/pytorch/vision/issues/3995#new-recipe\",\n            \"_metrics\": {\n                \"ImageNet-1K\": {\n                    \"acc@1\": 82.284,\n                    \"acc@5\": 96.002,\n                }\n            },\n            \"_ops\": 11.514,\n            \"_file_size\": 230.474,\n            \"_docs\": \"\"\"\n                These weights improve upon the results of the original paper by using TorchVision's `new training recipe\n                <https://pytorch.org/blog/how-to-train-state-of-the-art-models-using-torchvision-latest-primitives/>`_.\n            \"\"\",\n        },\n    )\n    DEFAULT = IMAGENET1K_V2\n\n\nclass ResNeXt50_32X4D_Weights(WeightsEnum):\n    IMAGENET1K_V1 = Weights(\n        url=\"https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth\",\n        transforms=partial(ImageClassification, crop_size=224),\n        meta={\n            **_COMMON_META,\n            \"num_params\": 25028904,\n            \"recipe\": \"https://github.com/pytorch/vision/tree/main/references/classification#resnext\",\n            \"_metrics\": {\n                \"ImageNet-1K\": {\n                    \"acc@1\": 77.618,\n                    \"acc@5\": 93.698,\n                }\n            },\n            \"_ops\": 4.23,\n            \"_file_size\": 95.789,\n            \"_docs\": \"\"\"These weights reproduce closely the results of the paper using a simple training recipe.\"\"\",\n        },\n    )\n    IMAGENET1K_V2 = Weights(\n        url=\"https://download.pytorch.org/models/resnext50_32x4d-1a0047aa.pth\",\n        transforms=partial(ImageClassification, crop_size=224, resize_size=232),\n        meta={\n            **_COMMON_META,\n            \"num_params\": 25028904,\n            \"recipe\": \"https://github.com/pytorch/vision/issues/3995#new-recipe\",\n            \"_metrics\": {\n                \"ImageNet-1K\": {\n                    \"acc@1\": 81.198,\n                    \"acc@5\": 95.340,\n                }\n            },\n            \"_ops\": 4.23,\n            \"_file_size\": 95.833,\n            \"_docs\": \"\"\"\n                These weights improve upon the results of the original paper by using TorchVision's `new training recipe\n                <https://pytorch.org/blog/how-to-train-state-of-the-art-models-using-torchvision-latest-primitives/>`_.\n            \"\"\",\n        },\n    )\n    DEFAULT = IMAGENET1K_V2\n\n\nclass ResNeXt101_32X8D_Weights(WeightsEnum):\n    IMAGENET1K_V1 = Weights(\n        url=\"https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth\",\n        transforms=partial(ImageClassification, crop_size=224),\n        meta={\n            **_COMMON_META,\n            \"num_params\": 88791336,\n            \"recipe\": \"https://github.com/pytorch/vision/tree/main/references/classification#resnext\",\n            \"_metrics\": {\n                \"ImageNet-1K\": {\n                    \"acc@1\": 79.312,\n                    \"acc@5\": 94.526,\n                }\n            },\n            \"_ops\": 16.414,\n            \"_file_size\": 339.586,\n            \"_docs\": \"\"\"These weights reproduce closely the results of the paper using a simple training recipe.\"\"\",\n        },\n    )\n    IMAGENET1K_V2 = Weights(\n        url=\"https://download.pytorch.org/models/resnext101_32x8d-110c445d.pth\",\n        transforms=partial(ImageClassification, crop_size=224, resize_size=232),\n        meta={\n            **_COMMON_META,\n            \"num_params\": 88791336,\n            \"recipe\": \"https://github.com/pytorch/vision/issues/3995#new-recipe-with-fixres\",\n            \"_metrics\": {\n                \"ImageNet-1K\": {\n                    \"acc@1\": 82.834,\n                    \"acc@5\": 96.228,\n                }\n            },\n            \"_ops\": 16.414,\n            \"_file_size\": 339.673,\n            \"_docs\": \"\"\"\n                These weights improve upon the results of the original paper by using TorchVision's `new training recipe\n                <https://pytorch.org/blog/how-to-train-state-of-the-art-models-using-torchvision-latest-primitives/>`_.\n            \"\"\",\n        },\n    )\n    DEFAULT = IMAGENET1K_V2\n\n\nclass ResNeXt101_64X4D_Weights(WeightsEnum):\n    IMAGENET1K_V1 = Weights(\n        url=\"https://download.pytorch.org/models/resnext101_64x4d-173b62eb.pth\",\n        transforms=partial(ImageClassification, crop_size=224, resize_size=232),\n        meta={\n            **_COMMON_META,\n            \"num_params\": 83455272,\n            \"recipe\": \"https://github.com/pytorch/vision/pull/5935\",\n            \"_metrics\": {\n                \"ImageNet-1K\": {\n                    \"acc@1\": 83.246,\n                    \"acc@5\": 96.454,\n                }\n            },\n            \"_ops\": 15.46,\n            \"_file_size\": 319.318,\n            \"_docs\": \"\"\"\n                These weights were trained from scratch by using TorchVision's `new training recipe\n                <https://pytorch.org/blog/how-to-train-state-of-the-art-models-using-torchvision-latest-primitives/>`_.\n            \"\"\",\n        },\n    )\n    DEFAULT = IMAGENET1K_V1\n\n\nclass Wide_ResNet50_2_Weights(WeightsEnum):\n    IMAGENET1K_V1 = Weights(\n        url=\"https://download.pytorch.org/models/wide_resnet50_2-95faca4d.pth\",\n        transforms=partial(ImageClassification, crop_size=224),\n        meta={\n            **_COMMON_META,\n            \"num_params\": 68883240,\n            \"recipe\": \"https://github.com/pytorch/vision/pull/912#issue-445437439\",\n            \"_metrics\": {\n                \"ImageNet-1K\": {\n                    \"acc@1\": 78.468,\n                    \"acc@5\": 94.086,\n                }\n            },\n            \"_ops\": 11.398,\n            \"_file_size\": 131.82,\n            \"_docs\": \"\"\"These weights reproduce closely the results of the paper using a simple training recipe.\"\"\",\n        },\n    )\n    IMAGENET1K_V2 = Weights(\n        url=\"https://download.pytorch.org/models/wide_resnet50_2-9ba9bcbe.pth\",\n        transforms=partial(ImageClassification, crop_size=224, resize_size=232),\n        meta={\n            **_COMMON_META,\n            \"num_params\": 68883240,\n            \"recipe\": \"https://github.com/pytorch/vision/issues/3995#new-recipe-with-fixres\",\n            \"_metrics\": {\n                \"ImageNet-1K\": {\n                    \"acc@1\": 81.602,\n                    \"acc@5\": 95.758,\n                }\n            },\n            \"_ops\": 11.398,\n            \"_file_size\": 263.124,\n            \"_docs\": \"\"\"\n                These weights improve upon the results of the original paper by using TorchVision's `new training recipe\n                <https://pytorch.org/blog/how-to-train-state-of-the-art-models-using-torchvision-latest-primitives/>`_.\n            \"\"\",\n        },\n    )\n    DEFAULT = IMAGENET1K_V2\n\n\nclass Wide_ResNet101_2_Weights(WeightsEnum):\n    IMAGENET1K_V1 = Weights(\n        url=\"https://download.pytorch.org/models/wide_resnet101_2-32ee1156.pth\",\n        transforms=partial(ImageClassification, crop_size=224),\n        meta={\n            **_COMMON_META,\n            \"num_params\": 126886696,\n            \"recipe\": \"https://github.com/pytorch/vision/pull/912#issue-445437439\",\n            \"_metrics\": {\n                \"ImageNet-1K\": {\n                    \"acc@1\": 78.848,\n                    \"acc@5\": 94.284,\n                }\n            },\n            \"_ops\": 22.753,\n            \"_file_size\": 242.896,\n            \"_docs\": \"\"\"These weights reproduce closely the results of the paper using a simple training recipe.\"\"\",\n        },\n    )\n    IMAGENET1K_V2 = Weights(\n        url=\"https://download.pytorch.org/models/wide_resnet101_2-d733dc28.pth\",\n        transforms=partial(ImageClassification, crop_size=224, resize_size=232),\n        meta={\n            **_COMMON_META,\n            \"num_params\": 126886696,\n            \"recipe\": \"https://github.com/pytorch/vision/issues/3995#new-recipe\",\n            \"_metrics\": {\n                \"ImageNet-1K\": {\n                    \"acc@1\": 82.510,\n                    \"acc@5\": 96.020,\n                }\n            },\n            \"_ops\": 22.753,\n            \"_file_size\": 484.747,\n            \"_docs\": \"\"\"\n                These weights improve upon the results of the original paper by using TorchVision's `new training recipe\n                <https://pytorch.org/blog/how-to-train-state-of-the-art-models-using-torchvision-latest-primitives/>`_.\n            \"\"\",\n        },\n    )\n    DEFAULT = IMAGENET1K_V2\n\n\n# @register_model()\n@handle_legacy_interface(weights=(\"pretrained\", ResNet18_Weights.IMAGENET1K_V1))\ndef resnet18(*, weights: Optional[ResNet18_Weights] = None, progress: bool = True, **kwargs: Any) -> ResNet:\n    \"\"\"ResNet-18 from `Deep Residual Learning for Image Recognition <https://arxiv.org/abs/1512.03385>`__.\n\n    Args:\n        weights (:class:`~torchvision.models.ResNet18_Weights`, optional): The\n            pretrained weights to use. See\n            :class:`~torchvision.models.ResNet18_Weights` below for\n            more details, and possible values. By default, no pre-trained\n            weights are used.\n        progress (bool, optional): If True, displays a progress bar of the\n            download to stderr. Default is True.\n        **kwargs: parameters passed to the ``torchvision.models.resnet.ResNet``\n            base class. Please refer to the `source code\n            <https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py>`_\n            for more details about this class.\n\n    .. autoclass:: torchvision.models.ResNet18_Weights\n        :members:\n    \"\"\"\n    weights = ResNet18_Weights.verify(weights)\n\n    return _resnet(BasicBlock, [2, 2, 2, 2], weights, progress, **kwargs)\n\n\n# @register_model()\n@handle_legacy_interface(weights=(\"pretrained\", ResNet34_Weights.IMAGENET1K_V1))\ndef resnet34(*, weights: Optional[ResNet34_Weights] = None, progress: bool = True, **kwargs: Any) -> ResNet:\n    \"\"\"ResNet-34 from `Deep Residual Learning for Image Recognition <https://arxiv.org/abs/1512.03385>`__.\n\n    Args:\n        weights (:class:`~torchvision.models.ResNet34_Weights`, optional): The\n            pretrained weights to use. See\n            :class:`~torchvision.models.ResNet34_Weights` below for\n            more details, and possible values. By default, no pre-trained\n            weights are used.\n        progress (bool, optional): If True, displays a progress bar of the\n            download to stderr. Default is True.\n        **kwargs: parameters passed to the ``torchvision.models.resnet.ResNet``\n            base class. Please refer to the `source code\n            <https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py>`_\n            for more details about this class.\n\n    .. autoclass:: torchvision.models.ResNet34_Weights\n        :members:\n    \"\"\"\n    weights = ResNet34_Weights.verify(weights)\n\n    return _resnet(BasicBlock, [3, 4, 6, 3], weights, progress, **kwargs)\n\n\n# @register_model()\n@handle_legacy_interface(weights=(\"pretrained\", ResNet50_Weights.IMAGENET1K_V1))\ndef resnet50(*, weights: Optional[ResNet50_Weights] = None, progress: bool = True, **kwargs: Any) -> ResNet:\n    \"\"\"ResNet-50 from `Deep Residual Learning for Image Recognition <https://arxiv.org/abs/1512.03385>`__.\n\n    .. note::\n       The bottleneck of TorchVision places the stride for downsampling to the second 3x3\n       convolution while the original paper places it to the first 1x1 convolution.\n       This variant improves the accuracy and is known as `ResNet V1.5\n       <https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch>`_.\n\n    Args:\n        weights (:class:`~torchvision.models.ResNet50_Weights`, optional): The\n            pretrained weights to use. See\n            :class:`~torchvision.models.ResNet50_Weights` below for\n            more details, and possible values. By default, no pre-trained\n            weights are used.\n        progress (bool, optional): If True, displays a progress bar of the\n            download to stderr. Default is True.\n        **kwargs: parameters passed to the ``torchvision.models.resnet.ResNet``\n            base class. Please refer to the `source code\n            <https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py>`_\n            for more details about this class.\n\n    .. autoclass:: torchvision.models.ResNet50_Weights\n        :members:\n    \"\"\"\n    weights = ResNet50_Weights.verify(weights)\n\n    return _resnet(Bottleneck, [3, 4, 6, 3], weights, progress, **kwargs)\n\n\n# @register_model()\n@handle_legacy_interface(weights=(\"pretrained\", ResNet101_Weights.IMAGENET1K_V1))\ndef resnet101(*, weights: Optional[ResNet101_Weights] = None, progress: bool = True, **kwargs: Any) -> ResNet:\n    \"\"\"ResNet-101 from `Deep Residual Learning for Image Recognition <https://arxiv.org/abs/1512.03385>`__.\n\n    .. note::\n       The bottleneck of TorchVision places the stride for downsampling to the second 3x3\n       convolution while the original paper places it to the first 1x1 convolution.\n       This variant improves the accuracy and is known as `ResNet V1.5\n       <https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch>`_.\n\n    Args:\n        weights (:class:`~torchvision.models.ResNet101_Weights`, optional): The\n            pretrained weights to use. See\n            :class:`~torchvision.models.ResNet101_Weights` below for\n            more details, and possible values. By default, no pre-trained\n            weights are used.\n        progress (bool, optional): If True, displays a progress bar of the\n            download to stderr. Default is True.\n        **kwargs: parameters passed to the ``torchvision.models.resnet.ResNet``\n            base class. Please refer to the `source code\n            <https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py>`_\n            for more details about this class.\n\n    .. autoclass:: torchvision.models.ResNet101_Weights\n        :members:\n    \"\"\"\n    weights = ResNet101_Weights.verify(weights)\n\n    return _resnet(Bottleneck, [3, 4, 23, 3], weights, progress, **kwargs)\n\n\n# @register_model()\n@handle_legacy_interface(weights=(\"pretrained\", ResNet152_Weights.IMAGENET1K_V1))\ndef resnet152(*, weights: Optional[ResNet152_Weights] = None, progress: bool = True, **kwargs: Any) -> ResNet:\n    \"\"\"ResNet-152 from `Deep Residual Learning for Image Recognition <https://arxiv.org/abs/1512.03385>`__.\n\n    .. note::\n       The bottleneck of TorchVision places the stride for downsampling to the second 3x3\n       convolution while the original paper places it to the first 1x1 convolution.\n       This variant improves the accuracy and is known as `ResNet V1.5\n       <https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch>`_.\n\n    Args:\n        weights (:class:`~torchvision.models.ResNet152_Weights`, optional): The\n            pretrained weights to use. See\n            :class:`~torchvision.models.ResNet152_Weights` below for\n            more details, and possible values. By default, no pre-trained\n            weights are used.\n        progress (bool, optional): If True, displays a progress bar of the\n            download to stderr. Default is True.\n        **kwargs: parameters passed to the ``torchvision.models.resnet.ResNet``\n            base class. Please refer to the `source code\n            <https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py>`_\n            for more details about this class.\n\n    .. autoclass:: torchvision.models.ResNet152_Weights\n        :members:\n    \"\"\"\n    weights = ResNet152_Weights.verify(weights)\n\n    return _resnet(Bottleneck, [3, 8, 36, 3], weights, progress, **kwargs)\n\n\n# @register_model()\n@handle_legacy_interface(weights=(\"pretrained\", ResNeXt50_32X4D_Weights.IMAGENET1K_V1))\ndef resnext50_32x4d(\n    *, weights: Optional[ResNeXt50_32X4D_Weights] = None, progress: bool = True, **kwargs: Any\n) -> ResNet:\n    \"\"\"ResNeXt-50 32x4d model from\n    `Aggregated Residual Transformation for Deep Neural Networks <https://arxiv.org/abs/1611.05431>`_.\n\n    Args:\n        weights (:class:`~torchvision.models.ResNeXt50_32X4D_Weights`, optional): The\n            pretrained weights to use. See\n            :class:`~torchvision.models.ResNext50_32X4D_Weights` below for\n            more details, and possible values. By default, no pre-trained\n            weights are used.\n        progress (bool, optional): If True, displays a progress bar of the\n            download to stderr. Default is True.\n        **kwargs: parameters passed to the ``torchvision.models.resnet.ResNet``\n            base class. Please refer to the `source code\n            <https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py>`_\n            for more details about this class.\n    .. autoclass:: torchvision.models.ResNeXt50_32X4D_Weights\n        :members:\n    \"\"\"\n    weights = ResNeXt50_32X4D_Weights.verify(weights)\n\n    _ovewrite_named_param(kwargs, \"groups\", 32)\n    _ovewrite_named_param(kwargs, \"width_per_group\", 4)\n    return _resnet(Bottleneck, [3, 4, 6, 3], weights, progress, **kwargs)\n\n\n# @register_model()\n@handle_legacy_interface(weights=(\"pretrained\", ResNeXt101_32X8D_Weights.IMAGENET1K_V1))\ndef resnext101_32x8d(\n    *, weights: Optional[ResNeXt101_32X8D_Weights] = None, progress: bool = True, **kwargs: Any\n) -> ResNet:\n    \"\"\"ResNeXt-101 32x8d model from\n    `Aggregated Residual Transformation for Deep Neural Networks <https://arxiv.org/abs/1611.05431>`_.\n\n    Args:\n        weights (:class:`~torchvision.models.ResNeXt101_32X8D_Weights`, optional): The\n            pretrained weights to use. See\n            :class:`~torchvision.models.ResNeXt101_32X8D_Weights` below for\n            more details, and possible values. By default, no pre-trained\n            weights are used.\n        progress (bool, optional): If True, displays a progress bar of the\n            download to stderr. Default is True.\n        **kwargs: parameters passed to the ``torchvision.models.resnet.ResNet``\n            base class. Please refer to the `source code\n            <https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py>`_\n            for more details about this class.\n    .. autoclass:: torchvision.models.ResNeXt101_32X8D_Weights\n        :members:\n    \"\"\"\n    weights = ResNeXt101_32X8D_Weights.verify(weights)\n\n    _ovewrite_named_param(kwargs, \"groups\", 32)\n    _ovewrite_named_param(kwargs, \"width_per_group\", 8)\n    return _resnet(Bottleneck, [3, 4, 23, 3], weights, progress, **kwargs)\n\n\n# @register_model()\n@handle_legacy_interface(weights=(\"pretrained\", ResNeXt101_64X4D_Weights.IMAGENET1K_V1))\ndef resnext101_64x4d(\n    *, weights: Optional[ResNeXt101_64X4D_Weights] = None, progress: bool = True, **kwargs: Any\n) -> ResNet:\n    \"\"\"ResNeXt-101 64x4d model from\n    `Aggregated Residual Transformation for Deep Neural Networks <https://arxiv.org/abs/1611.05431>`_.\n\n    Args:\n        weights (:class:`~torchvision.models.ResNeXt101_64X4D_Weights`, optional): The\n            pretrained weights to use. See\n            :class:`~torchvision.models.ResNeXt101_64X4D_Weights` below for\n            more details, and possible values. By default, no pre-trained\n            weights are used.\n        progress (bool, optional): If True, displays a progress bar of the\n            download to stderr. Default is True.\n        **kwargs: parameters passed to the ``torchvision.models.resnet.ResNet``\n            base class. Please refer to the `source code\n            <https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py>`_\n            for more details about this class.\n    .. autoclass:: torchvision.models.ResNeXt101_64X4D_Weights\n        :members:\n    \"\"\"\n    weights = ResNeXt101_64X4D_Weights.verify(weights)\n\n    _ovewrite_named_param(kwargs, \"groups\", 64)\n    _ovewrite_named_param(kwargs, \"width_per_group\", 4)\n    return _resnet(Bottleneck, [3, 4, 23, 3], weights, progress, **kwargs)\n\n\n# @register_model()\n@handle_legacy_interface(weights=(\"pretrained\", Wide_ResNet50_2_Weights.IMAGENET1K_V1))\ndef wide_resnet50_2(\n    *, weights: Optional[Wide_ResNet50_2_Weights] = None, progress: bool = True, **kwargs: Any\n) -> ResNet:\n    \"\"\"Wide ResNet-50-2 model from\n    `Wide Residual Networks <https://arxiv.org/abs/1605.07146>`_.\n\n    The model is the same as ResNet except for the bottleneck number of channels\n    which is twice larger in every block. The number of channels in outer 1x1\n    convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048\n    channels, and in Wide ResNet-50-2 has 2048-1024-2048.\n\n    Args:\n        weights (:class:`~torchvision.models.Wide_ResNet50_2_Weights`, optional): The\n            pretrained weights to use. See\n            :class:`~torchvision.models.Wide_ResNet50_2_Weights` below for\n            more details, and possible values. By default, no pre-trained\n            weights are used.\n        progress (bool, optional): If True, displays a progress bar of the\n            download to stderr. Default is True.\n        **kwargs: parameters passed to the ``torchvision.models.resnet.ResNet``\n            base class. Please refer to the `source code\n            <https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py>`_\n            for more details about this class.\n    .. autoclass:: torchvision.models.Wide_ResNet50_2_Weights\n        :members:\n    \"\"\"\n    weights = Wide_ResNet50_2_Weights.verify(weights)\n\n    _ovewrite_named_param(kwargs, \"width_per_group\", 64 * 2)\n    return _resnet(Bottleneck, [3, 4, 6, 3], weights, progress, **kwargs)\n\n\n# @register_model()\n@handle_legacy_interface(weights=(\"pretrained\", Wide_ResNet101_2_Weights.IMAGENET1K_V1))\ndef wide_resnet101_2(\n    *, weights: Optional[Wide_ResNet101_2_Weights] = None, progress: bool = True, **kwargs: Any\n) -> ResNet:\n    \"\"\"Wide ResNet-101-2 model from\n    `Wide Residual Networks <https://arxiv.org/abs/1605.07146>`_.\n\n    The model is the same as ResNet except for the bottleneck number of channels\n    which is twice larger in every block. The number of channels in outer 1x1\n    convolutions is the same, e.g. last block in ResNet-101 has 2048-512-2048\n    channels, and in Wide ResNet-101-2 has 2048-1024-2048.\n\n    Args:\n        weights (:class:`~torchvision.models.Wide_ResNet101_2_Weights`, optional): The\n            pretrained weights to use. See\n            :class:`~torchvision.models.Wide_ResNet101_2_Weights` below for\n            more details, and possible values. By default, no pre-trained\n            weights are used.\n        progress (bool, optional): If True, displays a progress bar of the\n            download to stderr. Default is True.\n        **kwargs: parameters passed to the ``torchvision.models.resnet.ResNet``\n            base class. Please refer to the `source code\n            <https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py>`_\n            for more details about this class.\n    .. autoclass:: torchvision.models.Wide_ResNet101_2_Weights\n        :members:\n    \"\"\"\n    weights = Wide_ResNet101_2_Weights.verify(weights)\n\n    _ovewrite_named_param(kwargs, \"width_per_group\", 64 * 2)\n    return _resnet(Bottleneck, [3, 4, 23, 3], weights, progress, **kwargs)\n\n\ndef get_resnet_model(cuda: bool, model_derivative: str, pretrained: bool = True, model_state_dict: dict = None, **kwargs):\n    models = {\n        \"resnet18\": resnet18,\n        \"resnet34\": resnet34,\n        \"resnet50\": resnet50,\n        \"resnet101\": resnet101,\n        \"resnet152\": resnet152\n    }\n    assert model_derivative in models.keys(), \"Your selected resnet model derivative is unavailable\"\n\n    model = models[model_derivative](weights='ResNet50_Weights.DEFAULT', **kwargs) if pretrained else models[model_derivative](**kwargs)\n\n    if model_state_dict:\n        print(\"Loading pretrained model...\")\n        model.load_state_dict(model_state_dict)\n        print(\"Finished.\")\n    else:\n        print(\"Initializing parameters...\")\n        for para in model.parameters():\n            if para.dim() > 1:\n                nn.init.xavier_uniform_(para)\n        print(\"Finished.\")\n\n    if cuda:\n        model = model.to(\"cuda\")\n    return model","metadata":{"_cell_guid":"290433ef-0ce2-40b2-9cca-381dc6ecca77","_uuid":"56ed0f72-37ab-458e-a762-8cfa34ab5f97","collapsed":false,"jupyter":{"outputs_hidden":false},"papermill":{"duration":2.893239,"end_time":"2024-02-19T07:15:18.059834","exception":false,"start_time":"2024-02-19T07:15:15.166595","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Utils","metadata":{"_cell_guid":"e69752a3-07ba-4cb2-98f7-397110038af4","_uuid":"abe9ff4b-15e9-45a2-96e1-ec995de06d9d","papermill":{"duration":0.007009,"end_time":"2024-02-19T07:15:18.074373","exception":false,"start_time":"2024-02-19T07:15:18.067364","status":"completed"},"tags":[]}},{"cell_type":"code","source":"### Early stopping\nclass EarlyStopper:\n    def __init__(self, PATIENCE=1, MIN_DELTA=0):\n        self.counter = 0\n        self.min_delta = MIN_DELTA\n        self.patience = PATIENCE  # Num of epoch that val loss is allowed to increase\n        self.min_val_loss = float('inf')\n\n    def check(self, val_loss: float):\n        if val_loss < self.min_val_loss:\n            self.min_val_loss = val_loss\n            self.counter = 0\n        elif val_loss > (self.min_val_loss + self.min_delta):\n            self.counter += 1\n            if self.counter >= self.patience:\n                return True\n        return False\n\n### Logger\nimport json\n\nfrom datetime import datetime\nfrom multimethod import multimethod\n\n\nclass Logger:\n    __time: datetime\n\n    def __init__(self, phase: str = \"train\"):\n        \"\"\"\n        phase: \"train\" || \"test\"\n        \"\"\"\n        self.__time = {f\"{phase.capitalize()} at\": datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")}\n\n\n    @multimethod\n    def write(self,  file: str, log_info: dict, writing_mode: str = \"a\") -> None:\n        \"\"\"\n        This is used for writing multiple time log from model training\n        \"\"\"\n        with open(file=file, mode=writing_mode, encoding=\"UTF-8\", errors=\"ignore\") as f:\n            f.write(json.dumps(dict(self.__time, **log_info), indent=4))\n            f.write(\",\\n\")\n        return None\n\n    @multimethod\n    def write(self,  file: str, log_info: str, writing_mode: str = \"w\") -> None:\n        \"\"\"\n        This is used for writing one-time log from model testing\n        \"\"\"\n        with open(file=file, mode=writing_mode, encoding=\"UTF-8\", errors=\"ignore\") as f:\n            f.write(log_info)\n        return None\n\n\n### Utils\nimport os\n\nfrom box import Box\nfrom typing import Tuple, Dict, List\n# from src.modelling.vgg import get_vgg_model\n# from src.modelling.resnet import get_resnet_model\n\nimport torch\nimport torcheval\n\nfrom torchsummary import summary\nfrom torchvision.datasets import ImageFolder\nfrom torchvision.transforms import InterpolationMode, Compose\nfrom torch.utils.data import DataLoader, random_split, Dataset\n\nfrom torch.optim import (\n    Adam,\n    AdamW,\n    NAdam,\n    RAdam,\n    SparseAdam,\n    Adadelta,\n    Adagrad,\n    Adamax,\n    ASGD,\n    RMSprop,\n    Rprop,\n    LBFGS,\n    SGD\n)\n\nfrom torch.nn.modules import (\n    NLLLoss,\n    NLLLoss2d,\n    CTCLoss,\n    KLDivLoss,\n    GaussianNLLLoss,\n    PoissonNLLLoss,\n    L1Loss,\n    MSELoss,\n    HuberLoss,\n    SmoothL1Loss,\n    CrossEntropyLoss,\n    BCELoss,\n    BCEWithLogitsLoss\n)\n\nfrom torcheval.metrics import (\n    BinaryAccuracy,\n    BinaryF1Score,\n    BinaryPrecision,\n    BinaryRecall,\n    BinaryConfusionMatrix,\n    MulticlassAccuracy,\n    MulticlassF1Score,\n    MulticlassPrecision,\n    MulticlassRecall,\n    MulticlassConfusionMatrix,\n    BinaryBinnedPrecisionRecallCurve,\n    MulticlassBinnedPrecisionRecallCurve,\n    BinaryPrecisionRecallCurve\n)\n\nfrom torch.optim.lr_scheduler import (\n    LambdaLR,\n    MultiplicativeLR,\n    StepLR,\n    MultiStepLR,\n    ConstantLR,\n    LinearLR,\n    ExponentialLR,\n    PolynomialLR,\n    CosineAnnealingLR,\n    CosineAnnealingWarmRestarts,\n    ChainedScheduler,\n    SequentialLR,\n    ReduceLROnPlateau,\n    OneCycleLR\n)\n\nfrom torchvision.transforms.v2 import (\n    # Color\n    ColorJitter,\n    Grayscale,\n    RandomAdjustSharpness,\n    RandomAutocontrast,\n    RandomChannelPermutation,\n    RandomEqualize,\n    RandomGrayscale,\n    RandomInvert,\n    RandomPhotometricDistort,\n    RandomPosterize,\n    RandomSolarize,\n\n    # Geometry\n    CenterCrop,\n    ElasticTransform,\n    FiveCrop,\n    Pad,\n    RandomAffine,\n    RandomCrop,\n    RandomHorizontalFlip,\n    RandomIoUCrop,\n    RandomPerspective,\n    RandomResize,\n    RandomResizedCrop,\n    RandomRotation,\n    RandomShortestSize,\n    RandomVerticalFlip,\n    RandomZoomOut,\n    Resize,\n    ScaleJitter,\n    TenCrop,\n\n    # Meta\n    ClampBoundingBoxes,\n    ConvertBoundingBoxFormat,\n\n    # Misc\n    ConvertImageDtype,\n    GaussianBlur,\n    Identity,\n    Lambda,\n    LinearTransformation,\n    Normalize,\n    SanitizeBoundingBoxes,\n    ToDtype,\n\n    # Temporal\n    UniformTemporalSubsample,\n\n    # Type conversion\n    PILToTensor,\n    ToImage,\n    ToPILImage,\n    ToPureTensor\n)\n\n\n__all__ = [\"get_dataset\", \"get_train_val_loader\", \"get_test_loader\", \"get_model_summary\",\n           \"init_loss\", \"init_lr_scheduler\", \"init_metrics\", \"init_model\", \"init_model_optimizer_start_epoch\"\n           ]\n\n\ndef get_model_summary(model: torch.nn.Module, input_size: Tuple, device: str):\n    return summary(model=model, input_size=input_size, device=device)\n\n\ndef get_transformation(transform_dict: Box = None) -> Compose:\n    available_transform = {\n        # Color\n        \"ColorJitter\": ColorJitter,\n        \"Grayscale\": Grayscale,\n        \"RandomAdjustSharpness\": RandomAdjustSharpness,\n        \"RandomAutocontrast\": RandomAutocontrast,\n        \"RandomChannelPermutation\": RandomChannelPermutation,\n        \"RandomEqualize\": RandomEqualize,\n        \"RandomGrayscale\": RandomGrayscale,\n        \"RandomInvert\": RandomInvert,\n        \"RandomPhotometricDistort\": RandomPhotometricDistort,\n        \"RandomPosterize\": RandomPosterize,\n        \"RandomSolarize\": RandomSolarize,\n\n        # Geometry\n        \"CenterCrop\": CenterCrop,\n        \"ElasticTransform\": ElasticTransform,\n        \"FiveCrop\": FiveCrop,\n        \"Pad\": Pad,\n        \"RandomAffine\": RandomAffine,\n        \"RandomCrop\": RandomCrop,\n        \"RandomHorizontalFlip\": RandomHorizontalFlip,\n        \"RandomIoUCrop\": RandomIoUCrop,\n        \"RandomPerspective\": RandomPerspective,\n        \"RandomResize\": RandomResize,\n        \"RandomResizedCrop\": RandomResizedCrop,\n        \"RandomRotation\": RandomRotation,\n        \"RandomShortestSize\": RandomShortestSize,\n        \"RandomVerticalFlip\": RandomVerticalFlip,\n        \"RandomZoomOut\": RandomZoomOut,\n        \"Resize\": Resize,\n        \"ScaleJitter\": ScaleJitter,\n        \"TenCrop\": TenCrop,\n\n        # Meta\n        \"ClampBoundingBoxes\": ClampBoundingBoxes,\n        \"ConvertBoundingBoxFormat\": ConvertBoundingBoxFormat,\n\n        # Misc\n        \"ConvertImageDtype\": ConvertImageDtype,\n        \"GaussianBlur\": GaussianBlur,\n        \"Identity\": Identity,\n        \"Lambda\": Lambda,\n        \"LinearTransformation\": LinearTransformation,\n        \"Normalize\": Normalize,\n        \"SanitizeBoundingBoxes\": SanitizeBoundingBoxes,\n        \"ToDtype\": ToDtype,\n\n        # Temporal\n        \"UniformTemporalSubsample\": UniformTemporalSubsample,\n\n        # Type conversion\n        \"PILToTensor\": PILToTensor,\n        \"ToImage\": ToImage,\n        \"ToPILImage\": ToPILImage,\n        \"ToPureTensor\": ToPureTensor\n    }\n\n    # Took from InterpolationMode of pytorch\n    available_interpolation = {\n        \"NEAREST\": InterpolationMode.NEAREST,\n        \"BILINEAR\": InterpolationMode.BILINEAR,\n        \"BICUBIC\": InterpolationMode.BICUBIC,\n        # For PIL compatibility\n        \"BOX\": InterpolationMode.BOX,\n        \"HAMMING\": InterpolationMode.HAMMING,\n        \"LANCZOS\": InterpolationMode.LANCZOS,\n    }\n\n    available_dtype = {\n        \"complex64\": torch.complex64,\n        \"complex128\": torch.complex128,\n        \"float16\": torch.float16,\n        \"float32\": torch.float32,\n        \"float64\": torch.float64,\n        \"uint8\": torch.uint8,\n        \"int8\": torch.int8,\n        \"int16\": torch.int16,\n        \"int32\": torch.int32,\n        \"int6\": torch.int64\n    }\n\n    if transform_dict is not None:\n        transform_lst: List[str] = transform_dict.NAME_LIST\n        args: Box = transform_dict.ARGS\n        # Verify transformation\n        for i in range(len(transform_lst)):\n            assert transform_lst[i] in available_transform.keys(), \"Your selected transform is unavailable\"\n\n            # Verify interpolation mode & replace str name to its corresponding func\n            if transform_lst[i] in (\"Resize\", \"RandomRotation\"):\n                assert args[str(i)].interpolation in available_interpolation.keys(), \"Your selected interpolation mode in unavailable\"\n                args[str(i)].interpolation = available_interpolation[args[str(i)].interpolation]\n\n            # Verify dtype & replace str name to its corresponding func\n            if transform_lst[i] == \"ToDtype\":\n                assert args[str(i)].dtype in available_dtype.keys(), \"Your selected dtype in unavailable\"\n                args[str(i)].dtype = available_dtype[args[str(i)].dtype]\n        compose: Compose = Compose([available_transform[transform_lst[i]](**args[str(i)]) for i in range(len(transform_lst))])\n    else:\n        compose: Compose = Compose([])\n    return compose\n\n\ndef get_dataset(root: str,\n                transform: Box = None,\n                target_transform: Box = None\n                ) -> Dataset:\n    \"\"\"\n    root: dataset dir\n    input_shape: CHW\n    transform: Dict of transformation name and its corresponding args\n    target_transform:                     //                          but for labels/ target\n    \"\"\"\n    return ImageFolder(root=root,\n                       transform=get_transformation(transform_dict=transform),\n                       target_transform=get_transformation(transform_dict=target_transform)\n                       )\n\n\ndef get_train_val_loader(dataset: Dataset,\n                         train_size: float, batch_size: int,\n                         seed: int, cuda: bool, num_workers=1\n                         ) -> Tuple[DataLoader, DataLoader]:\n    train_size = round(len(dataset) * train_size)\n    pin_memory = True if cuda is True else False  # Use page-locked or not\n\n    train_set, validation_set = random_split(dataset=dataset,\n                                             generator=torch.Generator().manual_seed(seed),\n                                             lengths=[train_size, len(dataset) - train_size])\n\n    train_set = DataLoader(dataset=train_set,\n                           batch_size=batch_size,\n                           shuffle=True,\n                           num_workers=num_workers,\n                           pin_memory=pin_memory\n                           )\n\n    validation_set = DataLoader(dataset=validation_set,\n                                batch_size=batch_size,\n                                shuffle=True,\n                                num_workers=num_workers,\n                                pin_memory=pin_memory\n                                )\n    return train_set, validation_set\n\n\ndef get_test_loader(dataset: Dataset, batch_size: int, cuda: bool, num_workers=1) -> DataLoader:\n    # Use page-locked or not\n    pin_memory = True if cuda is True else False\n    return DataLoader(dataset=dataset,\n                      batch_size=batch_size,\n                      shuffle=True,\n                      num_workers=num_workers,\n                      pin_memory=pin_memory\n                      )\n##########################################################################################################################\n\n\ndef init_loss(name: str, args: Dict) -> torch.nn.Module:\n    available_loss = {\n        \"NLLLoss\": NLLLoss, \"NLLLoss2d\": NLLLoss2d,\n        \"CTCLoss\": CTCLoss, \"KLDivLoss\": KLDivLoss,\n        \"GaussianNLLLoss\": GaussianNLLLoss, \"PoissonNLLLoss\": PoissonNLLLoss,\n        \"CrossEntropyLoss\": CrossEntropyLoss, \"BCELoss\": BCELoss, \"BCEWithLogitsLoss\": BCEWithLogitsLoss,\n        \"L1Loss\": L1Loss, \"MSELoss\": MSELoss, \"HuberLoss\": HuberLoss, \"SmoothL1Loss\": SmoothL1Loss,\n    }\n    assert name in available_loss.keys(), \"Your selected loss function is unavailable\"\n    loss: torch.nn.Module = available_loss[name](**args)\n    return loss\n\n\ndef init_lr_scheduler(name: str, args: Dict, optimizer: torch.optim.Optimizer) -> torch.optim.lr_scheduler.LRScheduler:\n    available_lr_scheduler = {\n        \"LambdaLR\": LambdaLR, \"MultiplicativeLR\": MultiplicativeLR, \"StepLR\": StepLR, \"MultiStepLR\": MultiStepLR,\n        \"ConstantLR\": ConstantLR,\n        \"LinearLR\": LinearLR, \"ExponentialLR\": ExponentialLR, \"PolynomialLR\": PolynomialLR,\n        \"CosineAnnealingLR\": CosineAnnealingLR,\n        \"CosineAnnealingWarmRestarts\": CosineAnnealingWarmRestarts, \"ChainedScheduler\": ChainedScheduler,\n        \"SequentialLR\": SequentialLR,\n        \"ReduceLROnPlateau\": ReduceLROnPlateau, \"OneCycleLR\": OneCycleLR\n    }\n    assert name in available_lr_scheduler.keys(), \"Your selected lr scheduler is unavailable\"\n    return available_lr_scheduler[name](optimizer, **args)\n\n\ndef init_metrics(name_lst: List[str], args: Dict, device: str) -> List[torcheval.metrics.Metric]:\n    available_metrics = {\n        \"BinaryAccuracy\": BinaryAccuracy,\n        \"BinaryF1Score\": BinaryF1Score,\n        \"BinaryPrecision\": BinaryPrecision,\n        \"BinaryRecall\": BinaryRecall,\n        \"BinaryConfusionMatrix\": BinaryConfusionMatrix,\n        \"BinaryPrecisionRecallCurve\": BinaryPrecisionRecallCurve,\n        \"BinaryBinnedPrecisionRecallCurve\": BinaryBinnedPrecisionRecallCurve,\n\n        \"MulticlassAccuracy\": MulticlassAccuracy,\n        \"MulticlassF1Score\": MulticlassF1Score,\n        \"MulticlassPrecision\": MulticlassPrecision,\n        \"MulticlassRecall\": MulticlassRecall,\n        \"MulticlassConfusionMatrix\": MulticlassConfusionMatrix,\n        \"MulticlassBinnedPrecisionRecallCurve\": MulticlassBinnedPrecisionRecallCurve\n    }\n\n    # check whether metrics available or not\n    for metric in name_lst:\n        assert metric in available_metrics.keys(), \"Your selected metric is unavailable\"\n\n    metrics: List[torcheval.metrics.Metric] = []\n    for i in range(len(name_lst)):\n        metrics.append(available_metrics[name_lst[i]](**args[str(i)]))\n\n    metrics = [metric.to(device) for metric in metrics]\n    return metrics\n\n\ndef init_model(device: str, pretrained: bool, base: str,\n               name: str, state_dict: dict, **kwargs) -> torch.nn.Module:\n    available_bases = {\n        \"vgg\": get_vgg_model,\n        \"resnet\": get_resnet_model\n    }\n    assert base in available_bases.keys(), \"Your selected base is unavailable\"\n    model: torch.nn.Module = available_bases[base](device, name, pretrained, state_dict, **kwargs)\n    return model\n\n\ndef init_optimizer(name: str, model_paras, state_dict: Dict = None, **kwargs) -> torch.optim.Optimizer:\n    available_optimizers = {\n        \"Adam\": Adam, \"AdamW\": AdamW, \"NAdam\": NAdam, \"Adadelta\": Adadelta, \"Adagrad\": Adagrad, \"Adamax\": Adamax,\n        \"RAdam\": RAdam, \"SparseAdam\": SparseAdam, \"RMSprop\": RMSprop, \"Rprop\": Rprop, \"ASGD\": ASGD, \"LBFGS\": LBFGS,\n        \"SGD\": SGD\n    }\n    assert name in available_optimizers.keys(), \"Your selected optimizer is unavailable.\"\n\n    # init optimizer\n    optimizer: torch.optim.Optimizer = available_optimizers[name](model_paras, **kwargs)\n\n    if state_dict is not None:\n        optimizer.load_state_dict(state_dict)\n    return optimizer\n\n\ndef init_model_optimizer_start_epoch(device: str,\n                                     checkpoint_load: bool, checkpoint_path: str, resume_name: str,\n                                     optimizer_name: str, optimizer_args: Dict,\n                                     model_base: str, model_name: str, model_args: Dict,\n                                     pretrained: bool = False\n                                     ) -> Tuple[int, torch.nn.Module, torch.optim.Optimizer]:\n    model_state_dict = None\n    optimizer_state_dict = None\n    start_epoch = 1\n\n    if checkpoint_load:\n        checkpoint = torch.load(f=os.path.join(checkpoint_path, resume_name), map_location=device)\n        start_epoch = checkpoint[\"epoch\"] + 1\n        model_state_dict = checkpoint[\"model_state_dict\"]\n        optimizer_state_dict = checkpoint[\"optimizer_state_dict\"]\n\n    model: torch.nn.Module = init_model(device=device, pretrained=pretrained, base=model_base,\n                                        name=model_name, state_dict=model_state_dict, **model_args\n                                        )\n\n    optimizer: torch.optim.Optimizer = init_optimizer(name=optimizer_name, model_paras=model.parameters(),\n                                                      state_dict=optimizer_state_dict, **optimizer_args\n                                                      )\n    return start_epoch, model, optimizer\n\n","metadata":{"_cell_guid":"1a6acf9a-5262-4013-baab-9345f67c4cba","_uuid":"0c1de7b6-c511-4a2e-bffd-d603aba63bc9","collapsed":false,"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.067781,"end_time":"2024-02-19T07:15:18.149114","exception":false,"start_time":"2024-02-19T07:15:18.081333","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Trainer","metadata":{"_cell_guid":"c73a1ef1-56d1-47b2-8287-57a82a79d88d","_uuid":"3025be1e-017e-4344-bde3-531ef6e1ff7a","papermill":{"duration":0.006781,"end_time":"2024-02-19T07:15:18.163010","exception":false,"start_time":"2024-02-19T07:15:18.156229","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import os\n\nfrom box import Box\nfrom tqdm import tqdm\nfrom time import sleep\nfrom typing import List, Dict\n# from src.utils.logger import Logger\n# from src.utils.early_stopping import EarlyStopper\n# from src.utils.utils import init_loss, init_metrics, init_lr_scheduler, init_model_optimizer_start_epoch\n\nimport torch\nimport torcheval\n\nfrom torch.nn.functional import sigmoid, softmax\nfrom torch.utils.data import DataLoader\n\n\nclass Trainer:\n    __options: Box\n    __train_log_path: str\n    __eval_log_path: str\n    __checkpoint_path: str\n    __device: str\n\n    __train_loader: DataLoader\n    __validation_loader: DataLoader\n\n    __early_stopper: EarlyStopper\n    __logger: Logger\n\n    __loss: torch.nn.Module\n    __optimizer: torch.optim.Optimizer\n    __lr_schedulers: torch.optim.lr_scheduler.LRScheduler\n    __start_epoch: int\n    __model: torch.nn.Module\n\n    __best_val_loss: float\n\n    def __init__(self, options: Box,\n                 train_log_path: str, eval_log_path: str, checkpoint_path: str,\n                 train_loader: DataLoader, val_loader: DataLoader\n                 ):\n        self.__options: Box = options\n        self.__train_log_path: str = train_log_path\n        self.__eval_log_path: str = eval_log_path\n        self.__checkpoint_path: str = checkpoint_path\n        self.__device: str = \"cuda\" if self.__options.MISC.CUDA else \"cpu\"\n\n        self.__train_loader: DataLoader = train_loader\n        self.__validation_loader: DataLoader = val_loader\n\n        self.__early_stopper: EarlyStopper = EarlyStopper(**self.__options.SOLVER.EARLY_STOPPING)\n        self.__logger: Logger = Logger()\n\n        self.__loss = init_loss(name=self.__options.SOLVER.LOSS.NAME, args=self.__options.SOLVER.LOSS.ARGS)\n        self.__start_epoch, self.__model, self.__optimizer = init_model_optimizer_start_epoch(device=self.__device,\n                                                                                              checkpoint_load=self.__options.CHECKPOINT.LOAD,\n                                                                                              checkpoint_path=checkpoint_path,\n                                                                                              resume_name=self.__options.CHECKPOINT.RESUME_NAME,\n                                                                                              optimizer_name=self.__options.SOLVER.OPTIMIZER.NAME,\n                                                                                              optimizer_args=self.__options.SOLVER.OPTIMIZER.ARGS,\n                                                                                              model_base=self.__options.SOLVER.MODEL.BASE,\n                                                                                              model_name=self.__options.SOLVER.MODEL.NAME,\n                                                                                              model_args=self.__options.SOLVER.MODEL.ARGS\n                                                                                              )\n        self.__lr_schedulers: torch.optim.lr_scheduler.LRScheduler = init_lr_scheduler(\n            name=self.__options.SOLVER.LR_SCHEDULER.NAME, args=self.__options.SOLVER.LR_SCHEDULER.ARGS,\n            optimizer=self.__optimizer)\n        self.__best_val_loss: float = self.__get_best_val_loss()\n\n    @classmethod\n    def __init_subclass__(cls):\n        \"\"\"Check indispensable args when instantiate Trainer\"\"\"\n        required_class_variables = [\n            \"__options\", \"__train_log_path\", \"__eval_log_path\", \"__checkpoint_path\", \"__train_loader\", \"__val_loader\"\n        ]\n        for var in required_class_variables:\n            if not hasattr(cls, var):\n                raise NotImplementedError(\n                    f'Class {cls} lacks required `{var}` class attribute'\n                )\n\n    # Setter & Getter\n    @property\n    def model(self):\n        return self.__model\n\n    # Public methods\n    def train(self, sleep_time: int = None, metric_in_train: bool = False) -> None:\n        \"\"\"\n        sleep_time: temporarily cease the training process\n        metric_in_train: compute metrics during training phase or not\n        \"\"\"\n        print(\"Start training model ...\")\n\n        for epoch in range(self.__start_epoch, self.__start_epoch + self.__options.EPOCH.EPOCHS):\n            print(\"Epoch:\", epoch)\n\n            for phase, dataset_loader, log_path in zip((\"train\", \"eval\"),\n                                                       (self.__train_loader, self.__validation_loader),\n                                                       (self.__train_log_path, self.__eval_log_path)):\n                # Preliminary setups\n                self.__model.train() if phase == \"train\" else self.__model.eval()\n                metrics: List[torcheval.metrics.Metric] = init_metrics(name_lst=self.__options.METRICS.NAME_LIST,\n                                                                       args=self.__options.METRICS.ARGS,\n                                                                       device=self.__device) if metric_in_train else None\n\n                # Epoch running\n                run_epoch_result: Dict = self.__run_epoch(phase=phase, epoch=epoch, dataset_loader=dataset_loader,\n                                                          metrics=metrics)\n\n                # Logging\n                self.__logger.write(log_path, {**{\"epoch\": epoch}, **run_epoch_result})\n\n                if phase == \"eval\":\n                    # Save checkpoint\n                    if self.__options.CHECKPOINT.SAVE:\n                        self.__save_checkpoint(epoch=epoch, val_loss=run_epoch_result[\"loss\"],\n                                               save_all=self.__options.CHECKPOINT.SAVE_ALL,\n                                               obj={\"epoch\": epoch, \"val_loss\": run_epoch_result[\"loss\"],\n                                                    \"model_state_dict\": self.__model.state_dict(),\n                                                    \"optimizer_state_dict\": self.__optimizer.state_dict()\n                                                    }\n                                               )\n\n                    # Early stopping checking\n                    if self.__options.MISC.APPLY_EARLY_STOPPING:\n                        if self.__early_stopper.check(val_loss=run_epoch_result[\"loss\"]):\n                            exit()\n\n                # Stop program in the meantime\n                if sleep_time is not None:\n                    sleep(sleep_time)\n        return None\n\n    # Private methods\n    def __run_epoch(self, phase: str, epoch: int, dataset_loader: DataLoader,\n                    metrics: List[torcheval.metrics.Metric] = None) -> Dict:\n        \"\"\"\n        phase: \"train\" || \"eval\"\n        dataset_loader: train_loader || val_loader\n        metrics: only available in eval phase\n\n        Notes: loss of last iter is taken as loss of that epoch\n        \"\"\"\n        num_class = self.__options.SOLVER.MODEL.ARGS.num_classes\n        total_loss = 0\n\n        # Epoch training\n        for index, batch in tqdm(enumerate(dataset_loader), total=len(dataset_loader), colour=\"cyan\",\n                                 desc=phase.capitalize()):\n            imgs, labels = batch[0].type(torch.FloatTensor).to(self.__device), batch[1]\n\n            # reset gradients prior to forward pass\n            self.__optimizer.zero_grad()\n\n            with torch.set_grad_enabled(phase == \"train\"):\n                # forward pass\n                pred_labels = self.__model(imgs)\n                if num_class == 1:\n                    # Shape: N1 -> N\n                    pred_labels = sigmoid(pred_labels).squeeze(dim=1)\n                    labels = labels.type(torch.FloatTensor).to(self.__device)\n                else:\n                    # Shape: NC -> N\n                    pred_labels = softmax(pred_labels, dim=1)\n\n                # Update loss\n                mini_batch_loss = self.__loss(pred_labels, labels)\n\n                # backprop + optimize only if in training phase\n                if phase == 'train':\n                    mini_batch_loss.backward()\n                    self.__optimizer.step()\n                    self.__lr_schedulers.step(epoch=epoch + index / len(dataset_loader))\n\n                # Update metrics only if eval phase\n                if metrics is not None:\n                    metrics = [metric.update(pred_labels, labels) for metric in metrics]\n            # Accumulate minibatch into total loss\n            total_loss += mini_batch_loss.item()\n\n        if metrics is not None:\n            metrics_name = self.__options.METRICS.NAME_LIST\n            metric_val = [metric.compute().item() for metric in metrics]\n            training_result = {**{\"loss\": total_loss / len(dataset_loader)},\n                               **{metric_name: value for metric_name, value in zip(metrics_name, metric_val)}\n                               }\n        else:\n            training_result = {\"loss\": total_loss / len(dataset_loader)}\n        return training_result\n\n    def __save_checkpoint(self, epoch: int, val_loss: float, obj: dict, save_all: bool = False) -> None:\n        \"\"\"\n        save_all:\n            True: save all trained epoch\n            False: save only last and the best trained epoch\n        Best_epoch is still saved in either save_all is True or False\n        \"\"\"\n        save_name = os.path.join(self.__checkpoint_path, f\"epoch_{epoch}.pt\")\n        torch.save(obj=obj, f=save_name)\n\n        # Save best checkpoint\n        if val_loss < self.__best_val_loss:\n            save_name = os.path.join(self.__checkpoint_path, f\"best_checkpoint.pt\")\n            torch.save(obj=obj, f=save_name)\n\n            # Update best accuracy\n            self.__best_val_loss = val_loss\n\n        if not save_all and epoch - 1 > 0:\n            # Remove previous epoch\n            os.remove(os.path.join(self.__checkpoint_path, f\"epoch_{epoch - 1}.pt\"))\n        return None\n\n    def __get_best_val_loss(self) -> float:\n        if \"best_checkpoint.pt\" in os.listdir(self.__checkpoint_path):\n            return torch.load(f=os.path.join(self.__checkpoint_path, \"best_checkpoint.pt\"))[\"val_loss\"]\n        else:\n            return 1e9","metadata":{"_cell_guid":"e6cc97f1-8675-4d3a-a354-ffa205bf141b","_uuid":"71ad7daa-4cd9-4a55-bef6-8bf677866518","papermill":{"duration":0.934173,"end_time":"2024-02-19T07:15:19.104220","exception":false,"start_time":"2024-02-19T07:15:18.170047","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Main","metadata":{"_cell_guid":"a88cea81-af7e-4250-994a-a2cf6c7cb724","_uuid":"958ff78a-dbba-4742-9bdf-0a86179d0c18","papermill":{"duration":0.007864,"end_time":"2024-02-19T07:15:19.119497","exception":false,"start_time":"2024-02-19T07:15:19.111633","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import os\nimport shutil\nfrom box import Box\n\n\nimport os\nimport commentjson\n\nfrom box import Box\n# from src.tools.train import Trainer\n# from src.tools.inference import inference\n# from src.utils.utils import get_train_val_loader, get_test_loader, get_dataset\n\nfrom torch.utils.data import DataLoader\n\n\ndef train() -> None:\n    checkpoint_path = os.path.join(os.getcwd(), \"checkpoints\", options.SOLVER.MODEL.NAME)\n    log_path = os.path.join(os.getcwd(), \"logs\", options.SOLVER.MODEL.NAME)\n\n    if not os.path.isdir(checkpoint_path):\n        os.makedirs(checkpoint_path, 0o777, True)\n        print(f\"Checkpoint dir for {options.SOLVER.MODEL.NAME} was created.\")\n\n    if not os.path.isdir(log_path):\n        os.makedirs(log_path, 0o777, True)\n        print(f\"Log dir checkpoint for {options.SOLVER.MODEL.NAME} was created.\")\n\n    train_log_path = os.path.join(log_path, f\"training_log.json\")\n    eval_log_path = os.path.join(log_path, f\"eval_log.json\")\n\n\n    train_set = get_dataset(root=os.path.join(\"/kaggle/input/celeb-a\", options.DATA.DATASET_NAME, \"train\"),\n                            transform=options.DATA.TRANSFORM)\n\n    train_loader, val_loader = get_train_val_loader(dataset=train_set,\n                                                    train_size=options.DATA.TRAIN_SIZE,\n                                                    batch_size=options.DATA.BATCH_SIZE, seed=options.MISC.SEED,\n                                                    cuda=options.MISC.CUDA, num_workers=options.DATA.NUM_WORKERS\n                                                    )\n    print(f\"\"\"Train batch: {len(train_loader)}, Validation batch: {len(val_loader)}\nTraining model {options.SOLVER.MODEL.NAME}\n\"\"\")\n\n    trainer = Trainer(options=options,\n                      train_log_path=train_log_path,\n                      eval_log_path=eval_log_path,\n                      checkpoint_path=checkpoint_path,\n                      train_loader=train_loader,\n                      val_loader=val_loader\n                      )\n    trainer.train(metric_in_train=True)\n    print(\"Training finished\")\n    return None\n\n\ndef test(option_path: str) -> None:\n    for dataset in ([\"celeb_A\", \"collected_v3\", \"collected_v4\"]):\n        options = Box(commentjson.loads(open(file=option_path, mode=\"r\").read()))\n        checkpoint_path = os.path.join(os.getcwd(), \"checkpoints\", options.MODEL.NAME, options.CHECKPOINT.NAME)\n\n        options.DATA.DATASET_NAME = dataset\n        log_path = os.path.join(os.getcwd(), \"logs\", options.MODEL.NAME, f\"testing_log_{dataset}.json\")\n\n        test_set = get_dataset(root=os.path.join(os.getcwd(), options.DATA.DATASET_NAME, \"test\"), transform=options.DATA.TRANSFORM)\n\n        test_loader: DataLoader = get_test_loader(dataset=test_set,\n                                                  batch_size=options.DATA.BATCH_SIZE,\n                                                  cuda=options.MISC.CUDA,\n                                                  num_workers=options.DATA.NUM_WORKERS\n                                                  )\n        print(f\"\"\"Test batch: {len(test_loader)}\"\"\")\n\n        inference(options=options, checkpoint_path=checkpoint_path, log_path=log_path, test_loader=test_loader, num_threshold=2)\n    return None\n\n\ndef main() -> None:\n    train()\n    # test(option_path=os.path.join(os.getcwd(), \"configs\", \"inference_config.json\"))\n    return None\n\n\nif __name__ == '__main__':\n    main()","metadata":{"_cell_guid":"9f28825b-6e78-40ee-ae6c-cecdeca661cc","_uuid":"b8f28d65-5c66-4e96-8541-d5e2fcde9802","collapsed":false,"jupyter":{"outputs_hidden":false},"papermill":{"duration":330.642629,"end_time":"2024-02-19T07:20:49.769194","exception":false,"start_time":"2024-02-19T07:15:19.126565","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]}]}