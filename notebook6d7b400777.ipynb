{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "is_executing": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for VGG:\n\tMissing key(s) in state_dict: \"features.14.weight\", \"features.14.bias\", \"features.16.weight\", \"features.16.bias\", \"features.19.weight\", \"features.19.bias\", \"features.21.weight\", \"features.21.bias\", \"features.23.weight\", \"features.23.bias\", \"features.25.weight\", \"features.25.bias\", \"features.28.weight\", \"features.28.bias\", \"features.30.weight\", \"features.30.bias\", \"features.32.weight\", \"features.32.bias\", \"features.34.weight\", \"features.34.bias\". \n\tUnexpected key(s) in state_dict: \"features.15.weight\", \"features.15.bias\", \"features.17.weight\", \"features.17.bias\", \"features.20.weight\", \"features.20.bias\", \"features.22.weight\", \"features.22.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 67\u001b[0m\n\u001b[0;32m     64\u001b[0m vgg_model \u001b[38;5;241m=\u001b[39m VGG(make_layers(cfg), DROPOUT\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, NUM_CLASSES\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, init_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     66\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mLocal\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mSource\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124msemester_6\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mface_attribute\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mcheckpoints\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mvgg13\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mbest_checkpoint.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 67\u001b[0m \u001b[43mvgg_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_state_dict\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\nn\\modules\\module.py:2152\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2147\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2148\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2149\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[0;32m   2151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2152\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2153\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   2154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for VGG:\n\tMissing key(s) in state_dict: \"features.14.weight\", \"features.14.bias\", \"features.16.weight\", \"features.16.bias\", \"features.19.weight\", \"features.19.bias\", \"features.21.weight\", \"features.21.bias\", \"features.23.weight\", \"features.23.bias\", \"features.25.weight\", \"features.25.bias\", \"features.28.weight\", \"features.28.bias\", \"features.30.weight\", \"features.30.bias\", \"features.32.weight\", \"features.32.bias\", \"features.34.weight\", \"features.34.bias\". \n\tUnexpected key(s) in state_dict: \"features.15.weight\", \"features.15.bias\", \"features.17.weight\", \"features.17.bias\", \"features.20.weight\", \"features.20.bias\", \"features.22.weight\", \"features.22.bias\". "
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self, features, DROPOUT=0.5, NUM_CLASSES=1000, init_weights=True):\n",
    "        super(VGG, self).__init__()\n",
    "        self.features = features\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 7 * 7, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(DROPOUT),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(DROPOUT),\n",
    "            nn.Linear(4096, 2048),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(DROPOUT),\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(DROPOUT),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(DROPOUT),\n",
    "            nn.Linear(512, NUM_CLASSES)\n",
    "        )\n",
    "        if init_weights:\n",
    "            self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNom2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "def make_layers(cfg, batch_norm=False):\n",
    "    layers = []\n",
    "    in_channels = 3\n",
    "    for v in cfg:\n",
    "        if v == 'M':\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "        else:\n",
    "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
    "            if batch_norm:\n",
    "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "            else:\n",
    "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "            in_channels = v\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "cfg = [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M']\n",
    "\n",
    "vgg_model = VGG(make_layers(cfg), DROPOUT=0.5, NUM_CLASSES=1, init_weights=False)\n",
    "\n",
    "checkpoint = torch.load(r\"D:\\Local\\Source\\python\\semester_6\\face_attribute\\checkpoints\\vgg13\\best_checkpoint.pt\")\n",
    "vgg_model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "vgg_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VN Express set evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T08:02:01.710455Z",
     "iopub.status.busy": "2024-02-21T08:02:01.710079Z",
     "iopub.status.idle": "2024-02-21T08:02:01.720738Z",
     "shell.execute_reply": "2024-02-21T08:02:01.719725Z",
     "shell.execute_reply.started": "2024-02-21T08:02:01.710428Z"
    }
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix\n",
    "from torchvision.transforms import transforms\n",
    "from torchvision.transforms import InterpolationMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T08:02:01.722740Z",
     "iopub.status.busy": "2024-02-21T08:02:01.722052Z",
     "iopub.status.idle": "2024-02-21T08:02:01.737288Z",
     "shell.execute_reply": "2024-02-21T08:02:01.736391Z",
     "shell.execute_reply.started": "2024-02-21T08:02:01.722707Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "folder = '/kaggle/input/test-data-version-0-4/Test'\n",
    "data = {'image_path': [], 'label': []}\n",
    "\n",
    "for filename in os.listdir(folder):\n",
    "    image_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if 'Female' in filename:\n",
    "        label = 0\n",
    "    elif 'Male' in filename:\n",
    "        label = 1\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    data['image_path'].append(image_path)\n",
    "    data['label'].append(label)\n",
    "\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T08:02:01.740329Z",
     "iopub.status.busy": "2024-02-21T08:02:01.739980Z",
     "iopub.status.idle": "2024-02-21T08:02:01.751804Z",
     "shell.execute_reply": "2024-02-21T08:02:01.750883Z",
     "shell.execute_reply.started": "2024-02-21T08:02:01.740290Z"
    }
   },
   "outputs": [],
   "source": [
    "from torchvision.transforms import v2\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import InterpolationMode\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset  \n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.data = dataframe\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.data.iloc[idx]['image_path']\n",
    "        label = self.data.iloc[idx]['label']\n",
    "\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "test_set = CustomDataset(dataframe=df,\n",
    "                            transform=v2.Compose([\n",
    "                                v2.Resize(size=(224, 224), interpolation=InterpolationMode.BICUBIC),\n",
    "                                v2.PILToTensor(),\n",
    "                                v2.ToDtype(torch.float32, scale=True),\n",
    "                                v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "                            ]))\n",
    "\n",
    "test_set = DataLoader(dataset=test_set,\n",
    "                      batch_size=32,\n",
    "                      shuffle=True,\n",
    "                      num_workers=2,\n",
    "                      pin_memory=True\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T08:02:01.753153Z",
     "iopub.status.busy": "2024-02-21T08:02:01.752831Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "# Assuming you have already created the DataLoader 'test_set'\n",
    "# Get the first batch\n",
    "batch_images, batch_labels = next(iter(test_set))\n",
    "\n",
    "# Function to unnormalize the images\n",
    "def unnormalize(img):\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406])\n",
    "    std = torch.tensor([0.229, 0.224, 0.225])\n",
    "    img = img * std[:, None, None] + mean[:, None, None]\n",
    "    return img\n",
    "\n",
    "# Function to show images\n",
    "def show_images(images, labels):\n",
    "    fig, axes = plt.subplots(4, 8, figsize=(15, 8))\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        image = unnormalize(images[i]).permute(1, 2, 0).numpy()\n",
    "        ax.imshow(image)\n",
    "        ax.axis('off')\n",
    "        ax.set_title(f\"Label: {labels[i]}\")\n",
    "\n",
    "# Show the images in the first batch\n",
    "show_images(batch_images, batch_labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "\n",
    "thresholds = torch.arange(0.1, 1.0, 0.1)\n",
    "best_f1_score = 0 \n",
    "vgg_model.to('cuda')\n",
    "\n",
    "for threshold in thresholds:\n",
    "    predictions = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for index, batch in tqdm(enumerate(test_set), total=len(test_set), desc=\"Test\"):\n",
    "            imgs, ground_truths = batch[0].type(torch.FloatTensor), batch[1].type(torch.FloatTensor)\n",
    "\n",
    "            imgs = imgs.to(\"cuda\")\n",
    "            ground_truths = ground_truths.to(\"cuda\").detach().cpu().numpy().tolist()\n",
    "            preds = torch.nn.functional.sigmoid(vgg_model(imgs)).squeeze().detach().cpu().numpy().tolist()\n",
    "            \n",
    "            for pred, ground_truth in zip(preds, ground_truths):\n",
    "                #female 0, male 1\n",
    "                label= 1 if pred>threshold else 0\n",
    "                predictions.append(label)\n",
    "                labels.append(ground_truth)\n",
    "\n",
    "    conf_matrix = confusion_matrix(labels, predictions)\n",
    "    macro_average_recall = recall_score(labels, predictions, average='macro', zero_division=1)\n",
    "    macro_average_precision = precision_score(labels, predictions, average='macro', zero_division=1)\n",
    "    macro_average_f1_score = 2 * (macro_average_precision * macro_average_recall) / (macro_average_precision + macro_average_recall)\n",
    "    print(f'Threshold: {round(threshold.item(),1)} Precision: {macro_average_precision} Recall: {macro_average_recall} F1-Score: {macro_average_f1_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for index, batch in tqdm(enumerate(test_set), total=len(test_set), desc=\"Test\"):\n",
    "        imgs, ground_truths = batch[0].type(torch.FloatTensor), batch[1].type(torch.FloatTensor)\n",
    "\n",
    "        imgs = imgs.to(\"cuda\")\n",
    "        ground_truths = ground_truths.to(\"cuda\").detach().cpu().numpy().tolist()\n",
    "        preds = torch.nn.functional.sigmoid(vgg_model(imgs)).squeeze().detach().cpu().numpy().tolist()\n",
    "\n",
    "        for pred, ground_truth in zip(preds, ground_truths):\n",
    "            #female 0, male 1\n",
    "            label= 1 if pred>0.9 else 0\n",
    "            predictions.append(label)\n",
    "            labels.append(ground_truth)\n",
    "\n",
    "conf_matrix = confusion_matrix(labels, predictions)\n",
    "macro_average_recall = recall_score(labels, predictions, average='macro', zero_division=1)\n",
    "macro_average_precision = precision_score(labels, predictions, average='macro', zero_division=1)\n",
    "macro_average_f1_score = 2 * (macro_average_precision * macro_average_recall) / (macro_average_precision + macro_average_recall)\n",
    "print(f'Threshold: {round(threshold.item(),1)} Precision: {macro_average_precision} Recall: {macro_average_recall} F1-Score: {macro_average_f1_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "fig=plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Class 0', 'Class 1'], yticklabels=['Class 0', 'Class 1'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "fig.savefig('/kaggle/working/CFS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fpr, tpr, _ = roc_curve(labels, predictions)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "fig1=plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='AUC = {:.2f}'.format(roc_auc))\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate (Recall)')\n",
    "plt.title('AUC-ROC Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "fig1.savefig('/kaggle/working/AUC')\n",
    "precision, recall, _ = precision_recall_curve(labels, predictions)\n",
    "\n",
    "fig2=plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall, precision, color='b', lw=2, label='Precision-Recall Curve')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "fig2.savefig('/kaggle/working/PRE-RE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4471278,
     "sourceId": 7666855,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4466299,
     "sourceId": 7660014,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4466295,
     "sourceId": 7660009,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30648,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
